{"question": "What is the layered abstraction established by the class that inherits from both the multiprecision floating-point type and the domain element trait that separates low-level arithmetic concerns from high-level domain semantics?", "answer": "The layered abstraction established by `RealElement`'s dual inheritance from `_mpf` and `DomainElement` separates low-level multiprecision arithmetic concerns from high-level domain algebra semantics through a clear architectural separation:\n\n**Low-level layer (`_mpf` inheritance):**\n- Provides multiprecision floating-point arithmetic operations (addition, subtraction, multiplication, division)\n- Handles precision management and rounding modes\n- Manages the internal representation of floating-point numbers via the `_mpf_` tuple format\n- Inherits from `mpmath.ctx_mp_python._mpf`, which is part of the mpmath library's low-level numerical computation infrastructure\n\n**High-level layer (`DomainElement` inheritance):**\n- Provides domain algebra semantics through the `parent()` method, which identifies the domain (e.g., `RealField`) that the element belongs to\n- Enables integration with SymPy's domain system for polynomial computations\n- Allows the element to participate in domain operations like conversion between domains, unification, and coercion\n- Implements the trait pattern that makes the element recognizable as a domain element throughout the polys module\n\n**The abstraction in practice:**\nThe `RealElement` class (defined in `sympy/polys/domains/mpelements.py`, lines 19-30) combines both layers:\n- It inherits arithmetic operations from `_mpf` (low-level)\n- It implements `parent()` from `DomainElement` (high-level), returning `self.context._parent` to identify its domain\n- This allows `RealElement` instances to be used both as multiprecision numbers (via `_mpf` methods) and as domain elements in SymPy's polynomial domain system (via `DomainElement` interface)\n\nThis separation allows the codebase to maintain a clean boundary between numerical computation concerns (handled by mpmath) and algebraic domain concerns (handled by SymPy's domain system), while allowing `RealElement` to participate in both worlds seamlessly."}
{"question": "What mechanism in the string representation generated by the test function that verifies matrix expression string representations ensures that matrix expression semantics are preserved when symbolic matrix dimensions are parameterized with symbolic integers rather than concrete values?", "answer": "The mechanism that preserves matrix expression semantics when dimensions are parameterized with symbolic integers is implemented in the `_print_Symbol` method of the `ReprPrinter` class in `sympy/printing/repr.py`.\n\nWhen `srepr()` is called on a `MatrixSymbol` with symbolic integer dimensions (e.g., `MatrixSymbol(\"A\", n, n)` where `n = symbols('n', integer=True)`), the printer recursively processes the dimension symbols. The `_print_Symbol` method (lines 222-234) preserves semantic information by:\n\n1. Accessing the `_assumptions_orig` attribute: The method retrieves the original assumptions dictionary from the Symbol object using `expr._assumptions_orig` (line 223).\n\n2. Including assumptions in the representation: If assumptions exist, they are formatted as keyword arguments in the string representation. For example, a Symbol with `integer=True` is represented as `Symbol('n', integer=True)` rather than just `Symbol('n')` (lines 232-234).\n\n3. Ensuring round-trip evaluation: This ensures that when the generated string is evaluated with `eval()`, the recreated Symbol object maintains the same assumptions (like `integer=True`), which is critical for preserving the semantic meaning that the dimension is an integer.\n\nIn the test `test_matrix_expressions()` in `sympy/printing/tests/test_repr.py` (lines 332-338), this mechanism is verified by checking that `srepr(A)` produces `\"MatrixSymbol(Str('A'), Symbol('n', integer=True), Symbol('n', integer=True))\"`, demonstrating that the `integer=True` assumption on the dimension symbol `n` is preserved in the string representation.\n\nThis preservation is essential because matrix operations and validations depend on dimension assumptions (e.g., checking if dimensions are integers, nonnegative, etc.), and losing these assumptions during serialization would break the semantic correctness of matrix expressions."}
{"question": "What is the integration mechanism of the function that converts symbolic Kronecker products to explicit matrices within the symbolic expression hierarchy that maintains separation between symbolic and concrete computation layers?", "answer": "The integration mechanism uses a canonicalization rule system that applies transformation rules to KroneckerProduct expressions during evaluation.\n\n**Integration Architecture:**\n\n1. **Rule-based canonicalization**: The `explicit_kronecker_product` function is integrated as one of four rules in the `canonicalize` function (lines 344-350 in `kronecker.py`):\n   ```python\n   rules = (unpack,\n            explicit_kronecker_product,\n            flatten,\n            extract_commutative)\n   \n   canonicalize = exhaust(condition(lambda x: isinstance(x, KroneckerProduct),\n                                    do_one(*rules)))\n   ```\n\n2. **Invocation path**: When `KroneckerProduct.doit()` is called (line 226-232), it:\n   - Recursively calls `doit()` on all arguments (if `deep=True`)\n   - Wraps the result in a new `KroneckerProduct` instance\n   - Passes it to `canonicalize()`, which applies the rules including `explicit_kronecker_product`\n\n3. **Conditional conversion logic**: The `explicit_kronecker_product` function (lines 336-341) maintains separation by:\n   - Checking if all arguments are `MatrixBase` instances (concrete matrices)\n   - If yes: calling `matrix_kronecker_product()` to compute the explicit matrix result\n   - If no: returning the symbolic `KroneckerProduct` unchanged\n\n**Separation of Concerns:**\n\n- **Symbolic layer** (`KroneckerProduct` class): Handles symbolic operations like `_eval_adjoint()`, `_eval_transpose()`, `_eval_inverse()`, structural equality checks, and shape computations without performing actual matrix arithmetic.\n\n- **Concrete computation layer** (`explicit_kronecker_product` + `matrix_kronecker_product`): Performs actual matrix multiplication and construction when all operands are concrete matrices.\n\n- **Integration point**: The canonicalization system acts as the bridge, allowing symbolic expressions to remain symbolic until explicitly evaluated, while automatically converting to concrete matrices when all components are concrete. This design enables symbolic manipulation and optimization before computation, while seamlessly transitioning to concrete computation when appropriate."}
{"question": "What is the separation enforced by the discrete conditional domain class between inequality constraint resolution and base domain set intersection?", "answer": "The discrete conditional domain class (`ConditionalDiscreteDomain` in `sympy/stats/drv.py`) enforces a two-step separation in its `set` property (lines 198-206):\n\n1. Inequality constraint resolution: First, it calls `reduce_rational_inequalities_wrap(self.condition, rv)` to convert the condition (e.g., `x > 2`) into a set/interval representation. This wrapper function (defined in `sympy/stats/crv.py`, lines 558-571) handles different condition types:\n   - Relational conditions (e.g., `x > 2`) → converted to intervals\n   - `Or` conditions → converted to unions of intervals\n   - `And` conditions → converted to intersections of intervals\n\n2. Domain set intersection: Second, it intersects the result with `self.fulldomain.set` using `.intersect()`, which represents the base domain set before the condition is applied.\n\nThe separation is explicit in the code structure:\n```python\nreturn reduce_rational_inequalities_wrap(self.condition, rv).intersect(self.fulldomain.set)\n```\n\nThis design separates:\n- The conversion of logical conditions into set-theoretic representations (inequality resolution)\n- The application of those constraints to the underlying domain (set intersection)\n\nThis pattern appears in both `ConditionalDiscreteDomain` (line 205-206) and `ConditionalContinuousDomain` (line 145-146 in `crv.py`), showing a consistent architectural approach across discrete and continuous domains in the SymPy statistics module."}
{"question": "What is the dependency mechanism of the characteristic function method in the logarithmic discrete distribution class on the base class framework that ensures consistency across discrete distribution classes?", "answer": "The dependency mechanism uses the Template Method pattern implemented in the base class for single-variable discrete distributions. Here's how it works:\n\n**Template Method Pattern in Base Class:**\n\nThe base class defines a public `characteristic_function` method (in `sympy/stats/drv.py`, lines 99-105) that implements the template:\n\n```python\ndef characteristic_function(self, t, **kwargs):\n    \"\"\" Characteristic function \"\"\"\n    if not kwargs:\n        cf = self._characteristic_function(t)\n        if cf is not None:\n            return cf\n    return self.compute_characteristic_function(**kwargs)(t)\n```\n\n**Hook Method for Subclasses:**\n\nThe base class provides a hook method `_characteristic_function` (line 96-97) that returns `None` by default. Subclasses like the logarithmic distribution class can override this method to provide optimized closed-form implementations. In the logarithmic distribution class (`sympy/stats/drv_types.py`, lines 377-379), this override provides:\n\n```python\ndef _characteristic_function(self, t):\n    p = self.p\n    return log(1 - p * exp(I*t)) / log(1 - p)\n```\n\n**Fallback to Default Computation:**\n\nIf a subclass doesn't override `_characteristic_function` or it returns `None`, the base class falls back to `compute_characteristic_function` (lines 86-94), which computes the characteristic function generically from the probability density function using summation:\n\n```python\ndef compute_characteristic_function(self, **kwargs):\n    \"\"\" Compute the characteristic function from the PDF.\n    Returns a Lambda.\n    \"\"\"\n    x, t = symbols('x, t', real=True, cls=Dummy)\n    pdf = self.pdf(x)\n    cf = summation(exp(I*t*x)*pdf, (x, self.set.inf, self.set.sup))\n    return Lambda(t, cf)\n```\n\n**Consistency Mechanism:**\n\nThis pattern ensures consistency across all discrete distribution classes because:\n\n1. **Unified Interface**: All subclasses inherit the same `characteristic_function` method signature and behavior\n2. **Optional Optimization**: Subclasses can provide efficient closed-form implementations via `_characteristic_function` without breaking the interface\n3. **Automatic Fallback**: Subclasses that don't provide optimized implementations automatically get correct behavior through PDF-based computation\n4. **Same Pattern for Other Methods**: The same template method pattern is used for other methods like `cdf` and `moment_generating_function`, ensuring consistent architecture\n\nAll discrete distribution classes in `drv_types.py` (FlorySchulzDistribution, GeometricDistribution, HermiteDistribution, LogarithmicDistribution, NegativeBinomialDistribution, PoissonDistribution, SkellamDistribution, YuleSimonDistribution, ZetaDistribution) inherit from the base class and follow this same dependency mechanism, ensuring consistent behavior across the entire discrete distribution framework."}
{"question": "What is the dependency chain between the Taylor series term computation method's use of the Bernoulli number function and the factorial function for the hyperbolic cosecant function's series expansion?", "answer": "The dependency chain for the hyperbolic cosecant function's (`csch`) Taylor series expansion works as follows:\n\n**Direct Dependencies:**\nThe `taylor_term` static method of the `csch` class (located in `sympy/functions/elementary/hyperbolic.py`, lines 1100-1116) directly depends on two combinatorial functions:\n\n1. **`bernoulli(n + 1)`**: Called from `sympy.functions.combinatorial.numbers.bernoulli`, which computes Bernoulli numbers using Ramanujan's formula with caching for efficiency.\n\n2. **`factorial(n + 1)`**: Called from `sympy.functions.combinatorial.factorials.factorial`, which computes factorials using the Prime-Swing algorithm for large values.\n\n**The Formula:**\nThe `taylor_term` method implements the formula:\n```python\nreturn 2 * (1 - 2**n) * bernoulli(n + 1) / factorial(n + 1) * x**n\n```\n\nThis formula generates the coefficients for odd-powered terms in the Taylor series expansion of `csch(x)`.\n\n**Call Chain:**\nThe dependency flows through the series expansion mechanism:\n- `csch(x).series()` or `csch(x)._eval_nseries()` calls\n- `taylor_term(i, arg, previous_term)` for each term `i`, which then calls\n- `bernoulli(n + 1)` and `factorial(n + 1)` to compute the coefficient\n\n**Impact on Correctness:**\nIf `bernoulli` or `factorial` produce incorrect values, the Taylor series coefficients for `csch` will be wrong. For example, the test in `test_hyperbolic.py` (line 423) verifies that `csch(x).series(x, 0, 10)` produces:\n```\n1/x - x/6 + 7*x**3/360 - 31*x**5/15120 + 127*x**7/604800 - 73*x**9/3421440 + O(x**10)\n```\n\nThese coefficients depend directly on the correctness of `bernoulli(2)`, `bernoulli(4)`, `bernoulli(6)`, etc., and the corresponding factorial values. Any errors in these combinatorial functions would propagate directly to the series expansion, making it mathematically incorrect."}
{"question": "What is the dependency mechanism of the hyperbolic cosecant class on its reciprocal base class and the attribute that establishes dependencies with the hyperbolic sine class?", "answer": "The dependency mechanism works through inheritance and delegation via a class attribute. The hyperbolic cosecant class (`csch`) inherits from `ReciprocalHyperbolicFunction`, which provides a framework for reciprocal hyperbolic functions. The dependency on the hyperbolic sine class (`sinh`) is established through the `_reciprocal_of` class attribute, which `csch` sets to `sinh` (line 1088 in `hyperbolic.py`).\n\nThe base class `ReciprocalHyperbolicFunction` uses `_reciprocal_of` extensively to delegate operations:\n\n1. **Evaluation** (`eval` method, lines 1002-1013): Calls `cls._reciprocal_of.eval(arg)` (which is `sinh.eval(arg)`) and returns `1/t` if the result is not None.\n\n2. **Method delegation** (`_call_reciprocal`, lines 1015-1018): Creates an instance of the reciprocal function and calls methods on it dynamically via `getattr(o, method_name)`.\n\n3. **Property access** (`_eval_is_extended_real`, line 1062): Accesses `self._reciprocal_of(self.args[0]).is_extended_real`, directly using properties of the `sinh` instance.\n\n4. **Complex number operations** (`as_real_imag`, line 1046): Computes `(1 / self._reciprocal_of(self.args[0])).as_real_imag()`, delegating to `sinh`'s implementation.\n\n5. **Series expansion** (`_eval_as_leading_term`, line 1059): Delegates to `(1/self._reciprocal_of(self.args[0]))._eval_as_leading_term()`.\n\n6. **Rewrite operations** (lines 1033-1043): Methods like `_eval_rewrite_as_exp` use `_rewrite_reciprocal`, which calls corresponding rewrite methods on `sinh` and returns the reciprocal.\n\nThis design means `csch` inherits behavior from `ReciprocalHyperbolicFunction` and delegates core operations to `sinh` via `_reciprocal_of`, creating a tight coupling where changes to `sinh`'s implementation (evaluation logic, property methods, rewrite methods, series expansion) directly affect `csch`'s behavior."}
{"question": "What is the mechanism in the normalization constant property that ensures the combined probability mass function integrates to unity given the partial sum of reciprocal integers and the natural logarithm ratio term?", "answer": "The normalization constant property (`Z`) in the robust soliton distribution class ensures that the combined probability mass function integrates to unity through a specific computational mechanism that matches the structure of the probability mass components.\n\nThe mechanism works as follows:\n\n1. **Harmonic Series Component**: The normalization constant computes a partial sum of reciprocal integers: `z = sum(1/i)` for `i` from 1 to `round(k/R) - 1`, where `R = c * log(k/delta) * k^0.5`. This harmonic series sum directly corresponds to the structure of the robust component (`tau`) in the probability mass function, which has terms of the form `R/(k*x)` for values of `x` from 1 to `round(k/R) - 1`.\n\n2. **Logarithmic Correction**: The normalization constant adds a logarithmic correction term `log(R/delta)` to the harmonic series sum. This correction term matches the special case in the robust component (`tau`) at `x = round(k/R)`, which has the value `R * log(R/delta)/k`.\n\n3. **Scaling Factor**: The combined sum (harmonic series + logarithmic correction) is multiplied by `R/k` and then added to 1: `Z = 1 + z * R/k`. The factor `R/k` scales the sum to properly normalize the robust component, while the `+1` accounts for the ideal soliton component (`rho`).\n\n4. **Unification**: When the probability mass function `(rho + tau)/Z` is computed, the normalization constant `Z` ensures that:\n   - The harmonic series structure in `Z` cancels out the corresponding harmonic terms in `tau`\n   - The logarithmic correction in `Z` cancels out the logarithmic term in `tau` at the boundary point\n   - The scaling factor `R/k` ensures proper normalization across the entire support\n\nThis mechanism is implemented in the `Z` property (lines 781-786 of `sympy/stats/frv_types.py`), which computes the normalization constant by iterating through the harmonic series range and adding the logarithmic correction, then scaling appropriately. The `pmf` method (lines 804-818) divides the combined `(rho + tau)` by this `Z` value to ensure the probability mass function sums to unity over the entire support set `{1, 2, ..., k}`."}
{"question": "What architectural separation does the base class for tuple-parameter special functions' derivative evaluation method maintain between tuple and scalar argument differentiation through derivative dispatch indexing?", "answer": "The base class for tuple-parameter special functions (`TupleParametersBase` in `sympy/functions/special/hyper.py`) maintains a clear architectural separation in its `_eval_derivative` method (lines 77-87) between tuple argument differentiation and scalar argument differentiation through a dual-path indexing mechanism.\n\n**Tuple Argument Differentiation Path (lines 80-84):**\nWhen the first two arguments (which are tuples) contain the differentiation symbol, the method iterates through `_diffargs` (the combined tuple parameters) and uses tuple indexing `(1, i)` to call `fdiff((1, i))`. This tuple index routes to parameter-specific differentiation logic. For example, in `meijerg.fdiff` (line 546-560), when `argindex != 3`, it extracts `argindex[1]` to identify which specific parameter in the tuple to differentiate.\n\n**Scalar Argument Differentiation Path (line 85):**\nThe scalar argument (the third argument, `self.args[2]`) is handled separately using scalar index `3` via `fdiff(3)`. This scalar index routes to argument-specific differentiation logic, which computes the derivative with respect to the function's main argument (e.g., `z` in hypergeometric functions).\n\n**Control Flow Through Indexing:**\nThe `fdiff` method acts as a dispatcher based on index type:\n- **Tuple indices** `(1, i)`: Route to `_diff_wrt_parameter(argindex[1])` for parameter differentiation\n- **Scalar index** `3`: Route to argument differentiation logic (e.g., `hyper.fdiff(3)` returns `fac*hyper(nap, nbq, self.argument)`)\n\nThis separation ensures that tuple parameters (which may contain multiple values) are differentiated element-wise through iteration, while the scalar argument is differentiated as a single entity, with the indexing mechanism cleanly routing control flow to the appropriate differentiation logic."}
{"question": "What are the interdependencies between the expression transformation methods that convert between circular and hyperbolic function families in the elementary functions module?", "answer": "The interdependencies between the expression transformation methods that convert between circular and hyperbolic function families in the elementary functions module create a complex network of bidirectional rewrite relationships. These interdependencies are implemented through `_eval_rewrite_as_*` methods in both `trigonometric.py` and `hyperbolic.py`.\n\n**Primary Interdependencies:**\n\n1. **Hyperbolic-to-Trigonometric Cross-Family Conversions:**\n   - `sinh._eval_rewrite_as_sin()` (line 300) calls `sin(I * arg)` - converts hyperbolic sine to trigonometric sine via complex argument\n   - `cosh._eval_rewrite_as_cos()` (line 495) calls `cos(I * arg, evaluate=False)` - converts hyperbolic cosine to trigonometric cosine\n   - `tanh._eval_rewrite_as_tan()` (line 753) calls `tan(I * arg, evaluate=False)` - converts hyperbolic tangent to trigonometric tangent\n   - Similar patterns exist for reciprocal functions: `csch._eval_rewrite_as_sin()`, `csch._eval_rewrite_as_csc()`, `sech._eval_rewrite_as_cos()`, `sech._eval_rewrite_as_sec()`\n\n2. **Within-Hyperbolic-Family Conversions:**\n   - `sinh._eval_rewrite_as_cosh()` (line 306) calls `cosh(arg + pi*I/2)` - converts sinh to cosh with phase shift\n   - `cosh._eval_rewrite_as_sinh()` (line 501) calls `sinh(arg + pi*I/2, evaluate=False)` - converts cosh to sinh with phase shift\n   - `tanh._eval_rewrite_as_sinh()` (line 759) calls `sinh(arg)/sinh(pi*I/2 - arg, evaluate=False)` - converts tanh to sinh\n   - `tanh._eval_rewrite_as_cosh()` (line 762) calls `cosh(pi*I/2 - arg, evaluate=False)/cosh(arg)` - converts tanh to cosh\n   - `coth._eval_rewrite_as_sinh()` (line 948) calls `sinh(pi*I/2 - arg, evaluate=False)/sinh(arg)` - converts coth to sinh\n   - `coth._eval_rewrite_as_cosh()` (line 951) calls `cosh(arg)/cosh(pi*I/2 - arg, evaluate=False)` - converts coth to cosh\n\n3. **Self-Referential Conversions:**\n   - `sinh._eval_rewrite_as_tanh()` (line 309) calls `tanh(S.Half*arg)` recursively\n   - `sinh._eval_rewrite_as_coth()` (line 313) calls `coth(S.Half*arg)` recursively\n   - `cosh._eval_rewrite_as_tanh()` (line 504) calls `tanh(S.Half*arg)**2` recursively\n   - `cosh._eval_rewrite_as_coth()` (line 508) calls `coth(S.Half*arg)**2` recursively\n\n**Circular Evaluation Chain Prevention:**\n\nThe codebase prevents infinite recursion through the `evaluate=False` parameter, which is strategically placed in critical rewrite methods. When `evaluate=False` is passed to function constructors (like `cos(I * arg, evaluate=False)`), it prevents automatic evaluation and rewriting, breaking potential circular chains.\n\n**Circular Dependencies That Would Create Infinite Loops Without `evaluate=False`:**\n\n1. **sinh ↔ cosh cycle**: `sinh._eval_rewrite_as_cosh()` → `cosh(arg + pi*I/2)` could trigger `cosh._eval_rewrite_as_sinh()` → `sinh(arg + pi*I/2)` → back to sinh\n2. **tanh ↔ sinh/cosh cycles**: `tanh._eval_rewrite_as_sinh()` → `sinh(arg)/sinh(pi*I/2 - arg)` could trigger recursive rewriting of the denominator\n3. **coth ↔ sinh/cosh cycles**: Similar to tanh, with reciprocal relationships\n4. **Cross-family cycles**: Hyperbolic functions rewriting to trigonometric functions could theoretically trigger reverse conversions through complex argument transformations\n\nThe `evaluate=False` parameter in methods like `cosh._eval_rewrite_as_sinh()` (line 502), `tanh._eval_rewrite_as_tan()` (line 754), and `coth._eval_rewrite_as_sinh()` (line 949) ensures that newly created function instances do not automatically trigger their own rewrite methods, thus preventing infinite recursion while maintaining the mathematical correctness of the transformations."}
{"question": "What mechanism in the tensor product contraction process ensures that data array computation produces identical results regardless of the order in which tensor operands are arranged?", "answer": "The mechanism that ensures tensor index ordering independence when computing data arrays across different operand permutations in the tensor product contraction process involves two key components:\n\n1. **Index Alignment via Permutation**: In the `_extract_data` method of the tensor addition class (which demonstrates the pattern), when extracting data arrays from multiple tensor operands, the system uses a static method `_get_indices_permutation` that computes a permutation mapping between each operand's index order and a reference index order (typically the first operand's indices). This permutation is then applied using `permutedims` to rearrange each operand's data array to align with the reference index order before combining them. This ensures that regardless of how operands are ordered, their arrays are normalized to the same index structure before computation.\n\n2. **Unified Index Structure Construction**: For tensor products specifically, the `_indices_to_free_dum` static method processes indices from all operands simultaneously, creating a unified index structure that identifies free indices and dummy (contracted) indices across the entire expression. This unified structure is independent of operand order, as it processes indices based on their identity rather than their position in the operand list.\n\n3. **Symmetry-Based Canonicalization**: The tensor symmetry specification (represented by the `TensorSymmetry` class) plays a crucial role in validating this invariant through the canonicalization process. When `canon_bp()` (canonicalization using Butler-Portugal algorithm) is called, it uses `components_canon_args()` to extract symmetry information (base and generators) from each tensor component. This symmetry data is passed to the `canonicalize` function, which ensures that tensor expressions differing only by symmetric index permutations are recognized as equivalent. This canonicalization validates that the index ordering independence is preserved - if two expressions are equivalent under symmetry, they must produce the same canonical form and thus the same data arrays regardless of operand order.\n\nThe combination of these mechanisms ensures that when computing data arrays, the system normalizes index structures across operands and validates equivalence through symmetry-aware canonicalization, maintaining consistency regardless of operand permutation."}
{"question": "What is the reconciliation mechanism in the class for joint probability distributions between the property accessing the second argument and the tuple naming only the first argument?", "answer": "The reconciliation mechanism in `JointDistributionHandmade` (located in `sympy/stats/joint_rv_types.py`) works by having the `set` property bypass the `NamedArgsMixin` attribute lookup and directly access `self.args[1]`.\n\n**The Mechanism:**\n\n1. **NamedArgsMixin behavior**: The class inherits from `JointDistribution`, which inherits from `NamedArgsMixin`. `NamedArgsMixin` (in `sympy/stats/rv.py`, lines 1586-1594) provides `__getattr__` that maps attribute names to argument indices using `_argnames.index(attr)`. Since `_argnames = ('pdf',)` only contains 'pdf', accessing `self.pdf` maps to `self.args[0]`.\n\n2. **Direct property access**: The `set` property (lines 93-95 in `joint_rv_types.py`) directly returns `self.args[1]`, bypassing `NamedArgsMixin`'s lookup mechanism entirely:\n   ```python\n   @property\n   def set(self):\n       return self.args[1]\n   ```\n\n3. **Implicit structural assumptions**: This design creates an implicit contract that:\n   - The class must always be instantiated with exactly 2 arguments: `(pdf, _set)`\n   - The first argument (index 0) is the probability density function and is named in `_argnames`\n   - The second argument (index 1) is the set/domain and is NOT named in `_argnames`, but must always be present\n   - The argument ordering is fixed: PDF first, set second\n\n**Evidence from the codebase:**\n\n- Instantiation pattern (line 141 in `joint_rv_types.py`): `JointDistributionHandmade(pdf, _set)` confirms the two-argument structure\n- Test case (line 1179 in `sympy/core/tests/test_args.py`): `JointDistributionHandmade(x1 + x2, S.Reals**2)` shows the expected usage\n- The base class `JointDistribution` (line 284 in `joint_rv.py`) also has `_argnames = ('pdf',)`, indicating this pattern is consistent across the hierarchy\n\nThis reconciliation allows the class to maintain compatibility with `NamedArgsMixin` for the PDF (enabling `self.pdf` access) while still supporting a second unnamed argument for the set through direct indexing, at the cost of requiring a fixed argument order."}
{"question": "How does the base wrapper class in the autowrap module propagate compilation failure exceptions through its code generation and compilation pipeline when external tools fail?", "answer": "The base wrapper class (`CodeWrapper` in `sympy/utilities/autowrap.py`) propagates compilation failure exceptions through a centralized error handling mechanism in the `_process_files` method.\n\n**Error Propagation Flow:**\n\n1. **Pipeline Execution** (`wrap_code` method, lines 150-177): The code generation pipeline executes sequentially:\n   - `_generate_code(routine, helpers)` - generates source code files\n   - `_prepare_files(routine)` - prepares wrapper-specific files (e.g., setup.py for Cython, or no-op for f2py)\n   - `_process_files(routine)` - executes external compilation commands\n   - Module import\n\n2. **Error Handling** (`_process_files` method, lines 179-189): When external compilation tools fail:\n   - The method executes the compilation command via `subprocess.check_output()` (line 183)\n   - If the command fails, Python raises `CalledProcessError`\n   - This exception is caught (line 184) and converted to `CodeWrapError` (lines 185-187)\n   - The `CodeWrapError` includes the failed command and its output: `\"Error while executing command: %s. Command output is:\\n%s\"`\n\n3. **Inheritance Pattern**: Both `CythonCodeWrapper` (line 234) and `F2PyCodeWrapper` (line 485) inherit from `CodeWrapper` and do not override `_process_files`. They inherit the same error handling, so there is no difference in error handling between these implementations - both use the base class's centralized exception propagation mechanism.\n\n4. **Exception Details**: The `CodeWrapError` exception (defined at line 109) is a simple Exception subclass that carries the error message containing the command string and stderr/stdout output from the failed compilation process, providing diagnostic information to users about what went wrong during compilation.\n\nThe error propagates upward from `_process_files` → `wrap_code` → caller, allowing the calling code to catch `CodeWrapError` and handle compilation failures appropriately."}
{"question": "How does the method that estimates transformation steps compute the distance between two hypergeometric functions while preserving their modulo-1 congruence class structure?", "answer": "The method that estimates transformation steps computes the distance between two hypergeometric functions while preserving their modulo-1 congruence class structure through the following process:\n\n**1. Initial Validation:**\n   - First checks if both functions have the same `gamma` value (number of negative integer upper parameters). If not, returns -1 indicating transformation is impossible (lines 559-560).\n\n**2. Modulo-1 Bucketing:**\n   - Groups parameters from both functions into \"buckets\" based on their value modulo 1 using the `_mod1` helper function (line 561-562)\n   - Creates separate buckets for upper parameters (`ap`) and lower parameters (`bq`) of both the source function (`self`) and target function (`func`)\n   - The `_mod1` function handles both numeric values (using `Mod(x, 1)`) and symbolic expressions by extracting the constant coefficient and applying modulo-1 to it (lines 85-96)\n\n**3. Congruence Class Matching:**\n   - For each modulo-1 congruence class (bucket), verifies that:\n     - The same congruence class exists in both functions\n     - Both functions have the same number of parameters in that congruence class\n   - If any mismatch is found, returns -1 (lines 567-569)\n\n**4. Distance Calculation:**\n   - Within each matching congruence class, sorts the parameters from both functions (lines 570-573)\n   - Pairs corresponding parameters (by sorted order) and computes the absolute difference between each pair\n   - Sums all these absolute differences across all parameters in all congruence classes (lines 574-575)\n   - Returns this total sum as the transformation distance estimate (line 577)\n\n**Key Design Principles:**\n   - The modulo-1 structure is preserved because parameters are only compared within the same congruence class (same `mod` value)\n   - Parameters with different modulo-1 values are never directly compared, maintaining the invariant structure\n   - The distance represents the total \"shift\" needed to transform parameters while keeping them within their respective congruence classes\n\nThis method is used in the `lookup_origin` function (line 869) to find the nearest origin formula from the knowledge base for expanding hypergeometric functions, selecting the formula with the smallest difficulty value (line 875)."}
{"question": "How does the LaTeX rendering method for symmetric difference set operations coordinate the precedence calculation helper function with the conditional parenthesization method to ensure correct operator precedence when rendering nested expressions?", "answer": "The LaTeX rendering method for symmetric difference set operations (`_print_SymmetricDifference` in `sympy/printing/latex.py`) coordinates these components as follows:\n\n1. **Getting parent operation precedence**: The method calls `precedence_traditional(u)` to get the precedence value of the symmetric difference operation itself. According to `sympy/printing/precedence.py`, `SymmetricDifference` has precedence value 10 (same as `PRECEDENCE['Xor']`), defined in `PRECEDENCE_TRADITIONAL`.\n\n2. **Processing each argument**: For each argument in `u.args`, the method calls `self.parenthesize(i, prec)`, passing the argument and the parent's precedence level.\n\n3. **Inside the parenthesize method** (`sympy/printing/latex.py`, lines 242-250):\n   - It calls `precedence_traditional(item)` again to get the precedence of each individual argument\n   - It compares the argument's precedence (`prec_val`) with the parent's precedence (`level`)\n   - If `prec_val < level` or (when `strict=False`) `prec_val <= level`, it wraps the argument in parentheses using `_add_parens()`, which adds `\\left(...\\right)` LaTeX delimiters\n   - Otherwise, it returns the argument's LaTeX representation without parentheses\n\n4. **Final rendering**: The parenthesized argument strings are joined with ` \\triangle ` to produce the final LaTeX output.\n\nThis coordination ensures that nested expressions with lower or equal precedence (like Union, Intersection, or other set operations) are properly parenthesized, while higher-precedence expressions (like atoms or functions) are not unnecessarily parenthesized. For example, `SymmetricDifference(Union(A, B), C)` would render as `(A \\cup B) \\triangle C` because Union has the same precedence (10) as SymmetricDifference, requiring parentheses to maintain correct mathematical meaning."}
{"question": "How does the two-argument function class that computes the logarithm of the sum of exponentiations ensure argument ordering consistency in its instance creation method?", "answer": "The two-argument function class that computes the logarithm of the sum of exponentiations ensures argument ordering consistency in its instance creation method by sorting the input arguments using `default_sort_key` before passing them to the parent `Function.__new__` method.\n\nSpecifically, in `sympy/codegen/numpy_nodes.py` (lines 44-45), the `__new__` method is implemented as:\n\n```python\ndef __new__(cls, *args):\n    return Function.__new__(cls, *sorted(args, key=default_sort_key))\n```\n\nThis sorting ensures that regardless of the order in which arguments are provided during instantiation (e.g., `logaddexp(x, y)` vs `logaddexp(y, x)`), the arguments stored in `self.args` are always in a canonical, sorted order determined by `default_sort_key`. This function (defined in `sympy/core/sorting.py`) produces a consistent ordering key based on the class type, arguments, exponents, and coefficients of SymPy expressions.\n\n**Implications for derivative computation:**\n\nThe argument ordering consistency has important implications for the `fdiff` method (the first derivative computation method). When computing derivatives, the method uses `argindex` (1-indexed) to determine which argument position to differentiate with respect to. The `fdiff` implementation (lines 47-57) handles this by conditionally swapping the assignment of variables:\n\n- When `argindex == 1`: it treats `self.args[0]` as the variable to differentiate with respect to (`wrt`) and `self.args[1]` as the other argument\n- When `argindex == 2`: it swaps them, treating `self.args[1]` as `wrt` and `self.args[0]` as the other argument\n\nThe derivative formula is `1/(1 + exp(other-wrt))`, which is mathematically correct regardless of which argument is being differentiated. Because `__new__` guarantees a consistent argument order, `fdiff` can reliably use positional indexing (`argindex`) to determine which argument to differentiate, and the conditional swapping ensures the correct variable assignment for the derivative formula.\n\nThis design ensures that `logaddexp(x, y)` and `logaddexp(y, x)` produce equivalent instances (as verified in `sympy/codegen/tests/test_rewriting.py` line 361), and derivatives computed via `fdiff` are mathematically correct regardless of the original argument order provided during instantiation."}
{"question": "How does the test validation infrastructure handle abstract tensor expression objects with pre-constructor argument transformations?", "answer": "The test validation infrastructure in `sympy/core/tests/test_args.py` handles abstract tensor expression objects through the `_test_args` function (lines 97-102). This function validates objects by checking two conditions:\n\n1. All arguments are Basic instances: `all(isinstance(arg, Basic) for arg in obj.args)`\n2. Object recreatability: `obj.func(*obj.args) == obj` (for objects with non-empty args)\n\nFor TensExpr specifically, the test `test_sympy__tensor__tensor__TensExpr()` (line 4744) is skipped with `@SKIP(\"abstract class\")` because TensExpr is an abstract base class. Instead, tests validate concrete implementations like TensAdd and TensMul.\n\nPre-constructor argument transformations occur in the `__new__` methods of TensAdd and TensMul:\n\n**TensAdd transformations** (lines 2459-2468 in `sympy/tensor/tensor.py`):\n- Applies `_sympify()` to convert arguments to SymPy objects\n- Flattens nested TensAdd/Add objects via `_tensAdd_flatten()` (lines 2547-2556)\n- Sorts arguments using `default_sort_key`\n\n**TensMul transformations** (lines 3437-3482):\n- Applies `_sympify()` to all arguments\n- Detects and resolves dummy index conflicts using `_dedupe_indices()` (lines 4213-4253) to rename conflicting indices\n- Flattens nested TensMul/Mul objects\n- Contracts indices via `_tensMul_contract_indices()` to handle Einstein summation\n\n**Tensor transformations** (lines 2886-2901):\n- Parses indices via `_parse_indices()` (lines 2963-2978), converting Symbol objects to TensorIndex instances\n- Builds index structure using `_IndexStructure.from_indices()`\n\nWhen `_test_args` validates these objects, it receives them after all transformations have been applied in their constructors. The validation ensures the transformed arguments are valid Basic instances and that the object can be recreated from its args, confirming compatibility with SymPy's expression tree structure."}
{"question": "How does the abstract base class for muscle activation dynamics models decouple symbolic equation representation from numerical solver implementations using abstract property interfaces?", "answer": "The abstract base class for muscle activation dynamics models (located in `sympy/physics/biomechanics/activation.py`) decouples symbolic representation from numerical solver implementations through a standardized abstract property interface that defines the structure of differential equations without specifying their concrete form.\n\n**The Abstract Property Interface:**\n\nThe base class defines multiple abstract properties that concrete subclasses must implement:\n\n1. **`M`** - An abstract property returning a square matrix representing the left-hand side coefficients of the linear ODE system `M(x, r, t, p) x' = F(x, r, t, p)`\n\n2. **`F`** - An abstract property returning a column matrix representing the right-hand side equations of the same system\n\n3. **`state_vars` (or `x`)** - An abstract property returning a column matrix of state variables (functions of time)\n\n4. **`input_vars` (or `r`)** - An abstract property returning a column matrix of input variables\n\n5. **`constants` (or `p`)** - An abstract property returning a column matrix of non-time-varying symbolic constants\n\n6. **`order`** - An abstract property returning the order of the differential equation\n\n7. **`rhs()`** - An abstract method returning the solution to the ODE system\n\n**How Decoupling Works:**\n\nThe decoupling mechanism operates at two levels:\n\n1. **Symbolic Representation Layer**: The abstract properties (`M`, `F`, `x`, `r`, `p`) provide a standardized interface for accessing the symbolic form of the differential equations. Concrete implementations like `ZerothOrderActivation` and `FirstOrderActivationDeGroote2016` provide different symbolic expressions, but they all conform to the same interface structure.\n\n2. **Solver Independence**: Numerical solvers can interact with any subclass of the base class by accessing these properties without needing to know:\n   - The specific mathematical form of the activation dynamics\n   - The order of the differential equation (accessed via `order`)\n   - How state variables are structured (accessed via `state_vars`)\n   - The concrete implementation details\n\n**Example from the Code:**\n\nFor `ZerothOrderActivation` (which has no differential equation), the `M` property returns an empty matrix `Matrix([])`, while `FirstOrderActivationDeGroote2016` returns `Matrix([Integer(1)])` for `M` and a complex symbolic expression for `F` involving hyperbolic tangent functions. Despite these differences, both can be used by the same solver code because they implement the same abstract property interface.\n\n**Benefits:**\n\n- **Extensibility**: New activation dynamics models can be added by subclassing and implementing the abstract properties\n- **Solver Agnostic**: Solvers work with the interface, not specific implementations\n- **Code Generation**: The standardized structure (especially the `constants` property) facilitates automatic code generation for numerical solvers, as mentioned in the docstrings\n- **Integration**: The interface allows seamless integration with other biomechanics components (like `MusculotendonBase`) that accept any `ActivationBase` subclass\n\nThis design pattern follows the Strategy pattern, where the abstract base class defines the strategy interface, and concrete classes provide different strategies (activation dynamics models) that can be used interchangeably by clients (solvers and other components)."}
{"question": "How does the three-dimensional axes rendering class in the pyglet plotting module maintain synchronization between coordinate range updates and tick mark recalculation when child plot object bounds contain infinite coordinate values?", "answer": "The three-dimensional axes rendering class maintains synchronization by coordinating both operations within a single method that checks for infinite values before performing either operation.\n\nIn `sympy/plotting/pygletplot/plot_axes.py`, the `adjust_bounds` method (lines 98-107) handles this coordination:\n\n```python\ndef adjust_bounds(self, child_bounds):\n    b = self._bounding_box\n    c = child_bounds\n    for i in range(3):\n        if abs(c[i][0]) is S.Infinity or abs(c[i][1]) is S.Infinity:\n            continue\n        b[i][0] = c[i][0] if b[i][0] is None else min([b[i][0], c[i][0]])\n        b[i][1] = c[i][1] if b[i][1] is None else max([b[i][1], c[i][1]])\n        self._bounding_box = b\n        self._recalculate_axis_ticks(i)\n```\n\nWhen infinite values are detected (line 102), the method uses `continue` to skip both the bounding box update (lines 104-106) and the tick recalculation call (line 107). This keeps them synchronized: both are skipped together when infinite values are present, and both are performed together when values are finite.\n\nThe separation of concerns is achieved by having `_recalculate_axis_ticks` as a separate method (lines 109-115), while `adjust_bounds` orchestrates both operations. This ensures that:\n1. Tick calculation logic is isolated in its own method\n2. Both operations are always performed or skipped together, maintaining consistency\n3. The bounding box state and tick marks remain synchronized\n\nChild plot objects (like `PlotCurve` and `PlotSurface`) initialize their bounds with `S.Infinity` and `S.NegativeInfinity` as placeholders. When these infinite values are passed to `adjust_bounds`, the method correctly skips updating that axis's bounds and ticks, preventing invalid coordinate ranges from affecting the visualization."}
{"question": "How does the column transformation method in the matrix reduction operations mixin class coordinate the entry computation function with the matrix instance creation method during column operations?", "answer": "The column transformation methods in the matrix reduction operations mixin class (MatrixReductions) coordinate the entry computation function with the matrix instance creation method (`_new`) through a lazy evaluation pattern.\n\nWhen performing column operations (like `_eval_col_op_multiply_col_by_const`, `_eval_col_op_swap`, or `_eval_col_op_add_multiple_to_other_col`), each method:\n\n1. Defines an entry computation function (a lambda) that computes each matrix element `(i, j)` based on the transformation logic. For example, in `_eval_col_op_multiply_col_by_const` (lines 211-216 of `matrices.py`):\n   ```python\n   def entry(i, j):\n       if j == col:\n           return k * self[i, j]\n       return self[i, j]\n   ```\n\n2. Passes this entry function to `self._new(self.rows, self.cols, entry)`, which delegates to the concrete matrix class's `_new` implementation.\n\n3. The `_new` method (or `_handle_creation_inputs` when processing creation inputs, as seen in lines 4354-4357 of `matrixbase.py`) detects when the third argument is a callable and evaluates it for each `(i, j)` position:\n   ```python\n   if isinstance(args[2], Callable):\n       op = args[2]\n       flat_list = []\n       for i in range(rows):\n           for j in range(cols):\n               flat_list.append(op(i, j))\n   ```\n\nThis design ensures that:\n- The entry function captures the transformation logic without materializing the matrix until `_new` is called\n- Different matrix backends (dense, sparse, immutable) can implement `_new` differently while maintaining the same interface\n- Matrix structural properties (shape, element types) are preserved because `_new` receives explicit dimensions and the entry function computes elements on-demand\n\nThe interaction maintains referential integrity because the entry function accesses `self[i, j]` during evaluation, ensuring that the original matrix's elements are correctly transformed according to the column operation semantics."}
{"question": "How should the expression simplification workflow decouple the bottom-up traversal applying normal methods from the function converting trigonometric and exponential forms to enable independent testing of simplifications involving imaginary unit operations without triggering unnecessary side effects in the rest of the symbolic computation framework?", "answer": "Based on the codebase exploration, the expression simplification workflow in SymPy's `simplify()` function (located in `sympy/simplify/simplify.py`) currently couples expression normalization with trigonometric-exponential transformations in a way that makes independent testing of complex number simplifications difficult.\n\n**Current Architecture:**\n\n1. **Expression Normalization** (line 660): The simplification pipeline applies normalization early via `_bottom_up(expr, lambda w: getattr(w, 'normal', lambda: w)())`, which traverses the expression tree bottom-up and calls the `normal()` method on each subexpression. This normalizes expressions before other simplifications.\n\n2. **Trigonometric-Exponential Transformation** (lines 746-747): Later in the pipeline, after various other simplifications (trigsimp, logcombine, etc.), the code checks if the expression contains trigonometric, hyperbolic, or exponential functions and applies `exptrigsimp(short)`. This function (defined in `sympy/simplify/trigsimp.py`, lines 571-662) converts between trigonometric and exponential forms, potentially introducing the imaginary unit `I` when rewriting trigonometric functions as exponentials.\n\n**The Coupling Problem:**\n\nThe coupling occurs because:\n- Normalization happens early (line 660) and affects the entire expression structure\n- `exptrigsimp()` can introduce complex numbers (see line 660 in `trigsimp.py`: `if not (newexpr.has(I) and not expr.has(I))`)\n- These transformations interact with other parts of the simplification pipeline (trigsimp at line 716-717, powsimp, cancel, etc.)\n- Testing complex number simplifications in isolation requires running the full pipeline, which triggers normalization and other side effects\n\n**Decoupling Strategy:**\n\nTo enable independent testing, the architecture should:\n\n1. **Separate normalization from transformation**: Make normalization optional or configurable, allowing it to be bypassed when testing trigonometric-exponential transformations in isolation.\n\n2. **Isolate exptrigsimp execution**: Allow `exptrigsimp()` to be called independently without requiring the full simplification pipeline, so complex number simplifications can be tested without normalization side effects.\n\n3. **Parameterize the pipeline**: Add flags to `simplify()` (or create a separate testing interface) that allow selective execution of pipeline stages, enabling tests to run only the trigonometric-exponential transformation without normalization.\n\n4. **Create a testing interface**: Implement a separate function or context manager that can execute `exptrigsimp()` in isolation with minimal dependencies, allowing complex number simplification tests to run without triggering the broader algebraic system's normalization logic.\n\nThe current implementation at line 660 applies normalization unconditionally, and the trigonometric-exponential transformation at lines 746-747 depends on the normalized state. Decoupling would require making normalization optional or creating a separate code path for testing that bypasses normalization while still allowing the trigonometric-exponential transformation to execute."}
{"question": "How does the operator that increments upper indices in hypergeometric functions enable composition of index-shifting transformations when chained with other differential operator subclasses in the expansion framework?", "answer": "The operator that increments upper indices enables composition through its polynomial representation in the differential operator `z*d/dz`. \n\nIn the implementation (lines 1009-1019), the operator stores `Poly(_x/ai + 1, _x)` where `_x` is a Dummy variable representing `z*d/dz` (line 672), and `ai` is the upper index parameter. This polynomial form allows sequential composition when operators are chained.\n\nThe `Operator.apply()` method (lines 976-999) evaluates the polynomial by:\n1. Extracting coefficients from `self._poly.all_coeffs()`\n2. Building a sequence of derivatives using the provided generator function\n3. Combining coefficients with derivatives to compute the result\n\nWhen multiple operators are chained via `apply_operators()` (lines 1504-1512), each operator's polynomial representation is evaluated sequentially. The polynomial form `_x/ai + 1` represents the linear differential operator `(z*d/dz)/ai + 1`, which when applied transforms the hypergeometric function's parameters correctly.\n\nThe composition works because:\n- Each operator's polynomial encodes its transformation as a polynomial in the differential operator\n- The `apply()` method interprets this polynomial to compute the actual transformation\n- When chained, operators are applied in reverse order (line 1510: `for o in reversed(ops)`), ensuring correct composition semantics\n- The polynomial representation allows algebraic manipulation while preserving the operator's effect on hypergeometric function parameters\n\nThis design enables the `devise_plan()` function (lines 1515-1649) to create sequences of operators that transform one hypergeometric function into another, which is then executed by `apply_operators()` in the `_hyperexpand()` function (lines 1954-2058) to perform the actual expansion."}
{"question": "How does the constructor method's commutative factor separation and subsequent dual state class compatibility verification ensure type safety when building outer product operators from multiplied quantum state expressions?", "answer": "The constructor method's type safety mechanism works through a two-stage validation process when handling multiplied quantum state expressions (like `2*k*b` where `k` is a ket and `b` is a bra).\n\n**Stage 1: Commutative Factor Separation**\n\nWhen the constructor receives expressions that are `Mul` instances (lines 409-410 in `sympy/physics/quantum/operator.py`), it calls `args_cnc()` on both the ket and bra expressions (lines 411-412). This method separates commutative factors (scalars, numbers) from non-commutative factors (the actual quantum state objects). For example, `Mul(2, k, 3)` would decompose into `ket_c = [2, 3]` (commutative) and `kets = [k]` (non-commutative).\n\n**Stage 2: Dual State Class Compatibility Verification**\n\nAfter extraction, the constructor validates that:\n1. There is exactly one ket and one bra in the non-commutative parts (lines 414-420)\n2. The ket's `dual_class()` method returns a class that matches the bra's actual class (line 422)\n\nThe `dual_class()` method is defined in `KetBase` (line 227 in `sympy/physics/quantum/state.py`) and returns `BraBase`, while each specific ket subclass (like `Qubit`, `Ket`) overrides it to return its corresponding bra class (like `QubitBra`, `Bra`). This ensures that only compatible dual pairs can form valid outer products - for instance, a `Ket` can only pair with a `Bra`, not with a `QubitBra`.\n\n**Type Safety Guarantee**\n\nThis two-stage process ensures type safety because:\n- The `args_cnc()` decomposition isolates the actual quantum state objects from any commutative multipliers, allowing the constructor to access the core ket/bra objects for validation\n- The `dual_class()` check enforces that the ket and bra belong to compatible dual state hierarchies, preventing type mismatches that would violate quantum mechanical principles\n- If validation fails at either stage, appropriate `TypeError` or `ValueError` exceptions are raised (lines 414-426)\n\nThe relationship between these stages is that `args_cnc()` must successfully extract exactly one ket and one bra before `dual_class()` validation can occur, making the decomposition a prerequisite for the compatibility check."}
{"question": "How does the CSE argument tracking class maintain consistency between its sequential value-number-to-value list mapping and sorted ordering semantics required by the value-ordered argument retrieval method?", "answer": "The CSE argument tracking class (`FuncArgTracker` in `sympy/simplify/cse_main.py`) maintains consistency through sequential, append-only value number assignment and a list where indices equal value numbers.\n\n**Sequential Value Number Assignment:**\nIn `get_or_add_value_number` (lines 251-260), value numbers are assigned sequentially and never reused:\n```python\nnvalues = len(self.value_numbers)\nvalue_number = self.value_numbers.setdefault(value, nvalues)\nif value_number == nvalues:\n    self.value_number_to_value.append(value)\n    self.arg_to_funcset.append(OrderedSet())\n```\nNew values get the current length as their number, and the value is appended to `value_number_to_value` at that index, ensuring `value_number_to_value[value_number] == value`.\n\n**Sorted Ordering Semantics:**\nThe `get_args_in_value_order` method (lines 244-249) relies on this invariant:\n```python\nreturn [self.value_number_to_value[argn] for argn in sorted(argset)]\n```\nIt sorts value numbers and looks them up in the list. Since numbers are assigned sequentially (0, 1, 2, ...), sorting by value number gives insertion order, providing deterministic ordering.\n\n**Consistency Guarantees:**\n1. Append-only updates: `value_number_to_value` is only appended to, never modified in place, so existing mappings remain stable.\n2. Non-reused value numbers: Once assigned, a value number is never reassigned, preventing index conflicts.\n3. Index-value correspondence: The invariant `value_number_to_value[value_number] == value` is maintained because values are appended at indices equal to their assigned numbers.\n\n**During CSE Operations:**\nDuring `match_common_args` (lines 365-457), `update_func_argset` modifies argument sets but does not touch `value_number_to_value`. New values added via `get_or_add_value_number` (e.g., line 428 when creating `Unevaluated` common functions) are appended, preserving consistency. This append-only design allows concurrent modifications to argument sets while keeping the value-number-to-value mapping stable and the sorted retrieval deterministic."}
{"question": "Why does the non-strict greater-than inequality class deliberately raise a TypeError when its boolean conversion method is invoked during chained comparison operations?", "answer": "Based on the codebase exploration, the non-strict greater-than inequality class (GreaterThan) deliberately raises a TypeError in its `__bool__` method (defined in the base `Relational` class at lines 526-531 of `sympy/core/relational.py`) because Python's chained comparison evaluation model fundamentally requires boolean coercion that cannot be overridden.\n\nWhen Python evaluates a chained comparison like `x > y > z`, it transforms it into `(x > y) and (y > z)` (as documented in lines 1125-1149 of `relational.py`). The `and` operator then coerces each side to a boolean by calling `__bool__()`. Since SymPy cannot determine the mathematical truth value of symbolic inequalities (e.g., whether `x > y` is true when `x` and `y` are symbolic), it deliberately raises a TypeError to prevent incorrect boolean evaluation.\n\nThe documentation (lines 1125-1154) explains that this is an unavoidable constraint: Python provides no mechanism to override the `and` operator or control its short-circuiting behavior. PEP 335 proposed changes to allow this, but it was officially closed in March 2012. The `__bool__` method implementation (lines 526-531) explicitly raises TypeError with the message \"cannot determine truth value of Relational\" to signal this limitation.\n\nThis design choice ensures that symbolic inequalities remain as symbolic Boolean objects rather than being incorrectly coerced to Python's `True` or `False`, which would lose their symbolic nature. Users must instead use SymPy's symbolic Boolean operators (`And`, `Or`, `Not`) or bitwise operators (`&`, `|`, `~`) to combine inequalities, as demonstrated in the documentation and test files."}
{"question": "Why does the array creation input handler in the base N-dimensional array class perform shape inference through recursive scanning of nested iterables rather than requiring explicit shape specification?", "answer": "The array creation input handler performs shape inference through recursive scanning of nested iterables to provide a convenient API that allows users to create arrays from nested Python lists without explicitly specifying the shape parameter.\n\nLooking at the implementation in `sympy/tensor/array/ndim_array.py`, the `_handle_ndarray_creation_inputs` class method (lines 216-260) checks if `shape` is `None` (line 220). When it is, and the input is an iterable (line 233), it calls `_scan_iterable_shape` (line 234) to recursively scan the nested structure and infer the dimensions.\n\nThe `_scan_iterable_shape` method (lines 197-213) recursively traverses nested iterables:\n- For non-iterable elements, it returns the element and an empty shape tuple\n- For iterables, it recursively processes each element, collects their shapes, and verifies they're consistent (line 207-208)\n- It builds the shape tuple by combining the length of the current level with the recursively determined inner shapes (line 211)\n\nThis design enables convenient array creation syntax like:\n```python\nMutableDenseNDimArray([[2, 3], [4, 5]])  # Shape (2, 2) inferred automatically\nMutableDenseNDimArray([[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]])  # Shape (2, 3, 2) inferred\n```\n\nAs shown in the documentation examples (lines 101-107), users can create arrays directly from nested lists without needing to specify `shape=(2, 2)` or `shape=(2, 3, 2)`. The method still supports explicit shape specification when provided (line 111 shows an example with a flat list and explicit shape), giving users flexibility.\n\nThe recursive approach handles arbitrary nesting depths automatically, making the API more intuitive for users working with nested data structures, which is common in mathematical and scientific computing contexts where SymPy is used."}
{"question": "Why does the abstract base class for N-dimensional arrays delegate object creation to the immutable dense storage implementation instead of providing its own initialization?", "answer": "The abstract base class for N-dimensional arrays delegates object creation to the immutable dense storage implementation because it cannot provide its own initialization logic due to architectural constraints.\n\n**Abstract Base Class Design:**\n`NDimArray` (defined in `sympy/tensor/array/ndim_array.py`, line 86) is an abstract base class that inherits from `Printable`, not `Basic`. It cannot be instantiated directly because it lacks concrete implementations of required methods. Specifically, its `__getitem__` method raises `NotImplementedError` with the message \"A subclass of NDimArray should implement __getitem__\" (line 147), indicating it's an abstract interface.\n\n**Delegation Implementation:**\nThe `__new__` method of `NDimArray` (lines 142-144) directly delegates to `ImmutableDenseNDimArray`:\n```python\ndef __new__(cls, iterable, shape=None, **kwargs):\n    from sympy.tensor.array import ImmutableDenseNDimArray\n    return ImmutableDenseNDimArray(iterable, shape, **kwargs)\n```\n\n**Architectural Constraints Necessitating This Design:**\n\n1. **Separation of Concerns:** The class hierarchy separates two orthogonal dimensions:\n   - **Mutability:** `ImmutableNDimArray` vs `MutableNDimArray` (both inherit from `NDimArray`)\n   - **Storage:** `DenseNDimArray` vs `SparseNDimArray` (both inherit from `NDimArray`)\n   \n   This creates four concrete implementations: `ImmutableDenseNDimArray`, `MutableDenseNDimArray`, `ImmutableSparseNDimArray`, and `MutableSparseNDimArray`. The base class cannot implement initialization because different storage types require different data structures (dense arrays use lists, sparse arrays use dictionaries).\n\n2. **Abstract Interface:** `NDimArray` defines the common interface and shared methods (like `_parse_index`, `_handle_ndarray_creation_inputs`, arithmetic operations) but cannot implement storage-specific methods. For example, `DenseNDimArray.__getitem__` (in `dense_ndim_array.py`, lines 24-70) accesses `self._array` (a list), while `SparseNDimArray.__getitem__` (in `sparse_ndim_array.py`, lines 16-62) accesses `self._sparse_array` (a dictionary).\n\n3. **Default Implementation Choice:** `ImmutableDenseNDimArray` is chosen as the default because:\n   - **Immutability:** It inherits from `ImmutableNDimArray`, which inherits from both `NDimArray` and `Basic` (SymPy's base class for expressions). This makes it hashable and consistent with SymPy's design philosophy where expressions are immutable.\n   - **Dense Storage:** Dense storage is the most common use case and provides better performance for most operations.\n   - **Convenience:** It provides the most convenient default for users who simply want to create an array without specifying mutability or storage type.\n\n4. **Consistent Pattern:** This delegation pattern is consistent across the hierarchy. `DenseNDimArray.__new__` (line 17-18 in `dense_ndim_array.py`) also delegates to `ImmutableDenseNDimArray`, and `SparseNDimArray.__new__` (line 13-14 in `sparse_ndim_array.py`) delegates to `ImmutableSparseNDimArray`, showing that intermediate abstract classes also cannot provide their own initialization.\n\nThe design ensures that `NDimArray` remains a pure abstract interface while providing a sensible default concrete implementation for users who don't need to specify mutability or storage characteristics explicitly."}
{"question": "Why does the test function that verifies deprecation warnings for the executable finder utility wrap the call to the deprecated function within the context manager that asserts deprecation warnings are emitted?", "answer": "The test wraps the call to the deprecated function within the `warns_deprecated_sympy()` context manager to verify that the deprecation warning is emitted correctly and that the deprecated behavior still functions.\n\nAccording to SymPy's deprecation policy (documented in `doc/src/contributing/deprecations.md`), tests for deprecated functionality must use `warns_deprecated_sympy()` for two purposes:\n\n1. To test that the warning is emitted correctly — The context manager asserts that a `SymPyDeprecationWarning` is raised when the deprecated function is called. If no warning is emitted, the test fails with \"DID NOT WARN\" (as seen in `sympy/testing/pytest.py` lines 387-388).\n\n2. To test that the deprecated behavior still actually functions — The test ensures the deprecated code continues to work during the deprecation period, allowing users to migrate gradually.\n\nThe implementation shows that `find_executable()` in `sympy/utilities/misc.py` (lines 261-275) calls `sympy_deprecation_warning()` when invoked, which emits a `SymPyDeprecationWarning`. The test in `sympy/utilities/tests/test_misc.py` (lines 146-148) wraps the call within `warns_deprecated_sympy()` to catch and verify this warning.\n\nThis pattern is required by SymPy's testing infrastructure: the test suite is configured to fail if a `SymPyDeprecationWarning` is issued anywhere except within a `warns_deprecated_sympy()` block (as stated in the deprecation policy documentation, line 324-326). This ensures that:\n- All deprecated code usage is isolated to explicit deprecation tests\n- The rest of the codebase uses non-deprecated alternatives\n- Deprecation warnings are properly tested and documented\n\nThe test serves as the only place in the codebase that uses the deprecated functionality, demonstrating both that the warning mechanism works and that the deprecated function still operates correctly during the deprecation period."}
{"question": "Why does repeated instantiation of the continuous probability distribution class representing Gaussian distributions with mean and standard deviation parameters in the method that returns the probability distribution for a given time index in the continuous-time stochastic process class that models Brownian motion impact memory allocation patterns?", "answer": "The repeated instantiation of `NormalDistribution` objects in the `distribution()` method of `WienerProcess` impacts memory allocation patterns because:\n\n1. **No Caching Mechanism**: The `distribution()` method (located at lines 2298-2302 in `sympy/stats/stochastic_process_types.py`) creates a new `NormalDistribution(0, sqrt(key))` object on every call without any caching. Unlike other methods in the codebase that use the `@cacheit` decorator (e.g., `_preprocess` at line 492, `transition_probabilities` at line 1592), the `distribution()` method has no memoization.\n\n2. **Frequent Object Creation**: Every time `WienerProcess` is queried at a time point (via `__call__` at line 323), it calls `self.distribution(time)`, which instantiates a new `NormalDistribution` object. This occurs even when querying the same time point multiple times, as shown in test cases (e.g., `test_stochastic_process.py` lines 660, 667-670) where the process is queried at multiple time points like `X(2)`, `X(3)`, `X(4)`, `X(6)`.\n\n3. **Basic Object Allocation**: `NormalDistribution` inherits from `SingleContinuousDistribution` (line 3208 in `sympy/stats/crv_types.py`), which ultimately inherits from `Basic`. Each instantiation creates a new Python object with its own memory allocation, storing the mean and standard deviation parameters (`_argnames = ('mean', 'std')`).\n\n4. **StochasticPSpace Storage**: Each `NormalDistribution` object is stored within a `StochasticPSpace` object (created at line 323), which itself is stored in a `RandomIndexedSymbol`. When querying at multiple time points, this creates a chain of new objects: `NormalDistribution` → `StochasticPSpace` → `RandomIndexedSymbol` for each time point.\n\n5. **Memory Accumulation**: In scenarios where the process is queried at many time points (common in stochastic process analysis), this pattern leads to linear memory growth proportional to the number of unique time queries, as each query allocates new distribution objects even when the underlying mathematical distribution parameters (mean=0, std=sqrt(time)) could be reused or cached.\n\nThe impact is particularly noticeable when performing joint distribution calculations or probability queries across multiple time points, as each time point query triggers a new object allocation cycle."}
{"question": "Why does the exception that signals failures in heuristic-based greatest common divisor computation strategies integrate into the base exception class for polynomial-related errors?", "answer": "The exception that signals failures in heuristic-based greatest common divisor computation strategies integrates into the base exception class for polynomial-related errors (`BasePolynomialError`) to enable structured error handling and fallback mechanisms within the polynomial computation framework.\n\n**Integration Purpose:**\n\n1. **Structured Exception Hierarchy**: By inheriting from `BasePolynomialError` (defined in `sympy/polys/polyerrors.py`), the exception becomes part of a unified error handling system for all polynomial operations. This allows code to catch all polynomial-related errors at once if needed, or handle specific error types individually.\n\n2. **Fallback Mechanism**: The integration enables a critical fallback pattern used throughout the GCD computation code. In `sympy/polys/euclidtools.py`, when heuristic GCD methods are attempted (lines 1544-1548, 1552-1556, 1580-1584, 1588-1592), the code catches `HeuristicGCDFailed` exceptions and automatically falls back to alternative methods:\n   - For rational number fields (`QQ`), it falls back to `dup_ff_prs_gcd` (polynomial remainder sequence)\n   - For integer domains (`ZZ`), it falls back to `dup_rr_prs_gcd` (polynomial remainder sequence)\n\n3. **Consistent Error Handling**: The exception hierarchy ensures that all polynomial computation errors follow the same pattern, making error handling predictable and maintainable. Other polynomial exceptions like `ModularGCDFailed`, `PolynomialDivisionFailed`, `ExactQuotientFailed`, etc., all inherit from the same base class.\n\n4. **Domain-Specific Error Classification**: The base class (`BasePolynomialError`) serves as a marker that distinguishes polynomial computation errors from other types of exceptions in SymPy, allowing for domain-specific error handling strategies.\n\n**Implementation Details:**\n\nThe exception is raised in heuristic GCD implementations when the algorithm exhausts its attempts (e.g., after `HEU_GCD_MAX = 6` evaluation points in `heuristicgcd.py` line 118, or in `euclidtools.py` lines 1273 and 1397). The try-except blocks in the GCD computation functions (lines 1547, 1555, 1583, 1591 in `euclidtools.py`) catch this exception silently and proceed with fallback algorithms, demonstrating how the exception hierarchy enables graceful degradation of computation strategies."}
{"question": "Why does repeated instantiation and code generation of pre-decrement operator objects impact code generation test suite performance across thousands of similar operator tests?", "answer": "Repeated instantiation and code generation of pre-decrement operator objects impacts performance due to:\n\n1. **Object instantiation overhead**: Each `PreDecrement(x)` call goes through `Basic.__new__` (in `sympy/core/basic.py`), which:\n   - Creates a new object instance\n   - Initializes `_mhash = None` (for lazy hash computation)\n   - Assigns `_assumptions = cls.default_assumptions`\n   - Stores `_args = args`\n\n2. **Hash computation**: On first access, `__hash__` computes `hash((type(self).__name__,) + self._hashable_content())` where `_hashable_content()` returns `self._args`. This computation happens when objects are used in sets/dicts or during equality checks.\n\n3. **Equality check overhead**: The test pattern `p.func(*p.args) == p` creates a new object and compares it. `__eq__` calls `_hashable_content()` on both objects, which returns tuples that must be compared.\n\n4. **Code generation overhead**: `ccode(p)` calls `_print_PreDecrement` (in `sympy/printing/c.py:612`), which:\n   - Accesses `expr.args` (extracting the argument)\n   - Recursively calls `self._print(arg)` to process the argument\n   - Constructs the string `'--(%s)' % self._print(arg)`\n\nWhen scaled across thousands of similar operator tests (PreIncrement, PostIncrement, PostDecrement, etc.), these per-object costs accumulate:\n- Each test creates multiple instances\n- Each instance triggers hash computation\n- Each equality check compares hashable content\n- Each code generation call processes args recursively\n\nThe overhead is multiplicative: if each test creates 2-3 objects and performs 2-3 operations per object, thousands of tests result in tens of thousands of object instantiations, hash computations, and code generation calls, leading to measurable performance degradation in the test suite."}
{"question": "Why does the elementwise function application method for N-dimensional arrays contain special-case logic for sparse arrays that filters out zero-valued results?", "answer": "The elementwise function application method for N-dimensional arrays contains special-case logic for sparse arrays that filters out zero-valued results to maintain the sparse representation's storage efficiency.\n\nThe special-case logic is located in `sympy/tensor/array/ndim_array.py` at lines 350-351:\n\n```python\nif isinstance(self, SparseNDimArray) and f(S.Zero) == 0:\n    return type(self)({k: f(v) for k, v in self._sparse_array.items() if f(v) != 0}, self.shape)\n```\n\nThis optimization applies only when:\n1. The array is a `SparseNDimArray` instance\n2. The function maps zero to zero (`f(S.Zero) == 0`)\n\nWhen both conditions are met, the method:\n- Iterates only over the stored non-zero entries in `_sparse_array` (a dictionary)\n- Applies the function to each stored value\n- Filters out any results that become zero (`if f(v) != 0`)\n- Returns a new sparse array with only non-zero results\n\nThis aligns with the broader sparse array optimization strategy:\n\n1. **Storage efficiency**: Sparse arrays store only non-zero values in `_sparse_array` (see `sparse_ndim_array.py` lines 118, 155). During creation, zeros are excluded: `if el != 0: sparse_array[i] = _sympify(el)`.\n\n2. **Maintaining sparsity**: When setting items to zero, they are removed from the dictionary (lines 179-182, 186-187 in `sparse_ndim_array.py`). This keeps the representation compact.\n\n3. **Avoiding unnecessary computation**: Without this optimization, applying a function would require iterating over all array positions (including implicit zeros), defeating the purpose of sparse storage. The special case preserves sparsity by only processing stored entries and filtering out results that become zero.\n\n4. **Conditional optimization**: The check `f(S.Zero) == 0` ensures the optimization is safe—if a function can produce non-zero from zero, all positions must be considered, so the method falls back to the general implementation that processes all elements.\n\nThis design maintains the sparse array's memory efficiency and computational advantages while ensuring correctness for functions that preserve or create zeros."}
{"question": "Why does the power decomposition method validate correct transformation of unit-numerator rational bases into inverted integer bases with negated exponents?", "answer": "The `as_base_exp()` method in `sympy/core/power.py` validates the correct decomposition to ensure mathematical correctness and maintain the reconstruction invariant. \n\nThe method implements a normalization transformation: when a power has a rational base with numerator 1 and denominator != 1 (e.g., (1/2)^x), it returns the inverted form (2, -x) instead of (1/2, x). This transformation is based on the mathematical identity (1/b)^e = b^(-e).\n\nThe validation ensures that the invariant `Pow(*expr.as_base_exp()) == expr` holds - meaning that reconstructing a power expression from its decomposed base and exponent returns an expression mathematically equivalent to the original. This is explicitly tested in `sympy/functions/elementary/tests/test_exponential.py` where the comment states: \"Pow(*expr.as_base_exp()) == expr invariant should hold.\"\n\nThe validation is necessary because:\n1. It ensures mathematical correctness of the decomposition transformation\n2. It maintains consistency across the codebase where `as_base_exp()` is used extensively (found in 156 locations) for pattern matching, simplification, and symbolic manipulation\n3. It prevents errors in downstream operations that rely on the decomposed form, such as in solvers, simplifiers, and series expansions\n\nThe specific check `if b.is_Rational and b.p == 1 and b.q != 1:` validates that only unit-numerator rationals (fractions like 1/2, 1/3, etc.) are transformed, ensuring the decomposition correctly handles this special case while leaving other rational bases (like 2/3) unchanged."}
{"question": "Why does repeated instantiation of the inverse tendon force-length curve function class within the test method that verifies object instantiation impact overall test suite performance when scaled across hundreds of biomechanical curve function tests?", "answer": "Based on my exploration of the SymPy repository, the repeated instantiation of `TendonForceLengthInverseDeGroote2016` objects in `test_instance` impacts test suite performance due to several memory allocation overhead factors:\n\n**1. Float Object Creation Overhead:**\nEach instantiation of `TendonForceLengthInverseDeGroote2016` requires creating multiple `Float` objects for the constants. In the `with_defaults` class method (lines 404-408 in `curve.py`), four new `Float` instances are created every time:\n```python\nc0 = Float('0.2')\nc1 = Float('0.995')\nc2 = Float('0.25')\nc3 = Float('33.93669377311689')\nreturn cls(fl_T, c0, c1, c2, c3)\n```\nWhen `test_instance` is called repeatedly across hundreds of biomechanical curve function tests, this creates thousands of `Float` objects, each requiring memory allocation and initialization.\n\n**2. Symbolic Expression Tree Construction:**\nEach `TendonForceLengthInverseDeGroote2016` instance stores its arguments in `self.args`, creating a symbolic expression tree structure. The class inherits from `Function` (via `CharacteristicCurveFunction`), which maintains these arguments as immutable tuples. Each instantiation creates new argument tuples and expression nodes, even when the same values are used repeatedly.\n\n**3. Test Method Isolation:**\nThe `test_instance` method (line 298-301 in `test_curve.py`) creates a fresh instance in each test:\n```python\ndef test_instance(self):\n    fl_T_inv = TendonForceLengthInverseDeGroote2016(self.fl_T, *self.constants)\n    assert isinstance(fl_T_inv, TendonForceLengthInverseDeGroote2016)\n```\nWhen this pattern is repeated across multiple test classes (`TestTendonForceLengthInverseDeGroote2016`, `TestTendonForceLengthDeGroote2016`, `TestFiberForceLengthPassiveDeGroote2016`, etc.), each test creates new instances without reusing previously allocated objects.\n\n**4. Symbol Creation:**\nThe test fixture (lines 284-290) creates new `Symbol` objects (`self.fl_T`, `self.c0`, etc.) for each test class, and these symbols are used in every instantiation. While symbols may be cached by SymPy's internals, the test structure doesn't leverage this optimization.\n\n**5. Scaling Effect:**\nThe test suite contains multiple characteristic curve function classes (7 different curve types), each with their own `test_instance` method. When scaled across hundreds of tests, the cumulative effect of:\n- Creating new `Float` constants for each instantiation\n- Building new symbolic expression trees\n- Allocating memory for function instances and their argument tuples\n- Garbage collection overhead from many short-lived objects\n\ncreates significant memory allocation overhead that impacts overall test suite performance, especially when running the full test suite sequentially.\n\n**Potential Optimization Strategies (for reference):**\nWhile not part of the \"why\" question, the codebase could benefit from:\n- Caching `Float` constants as module-level or class-level variables\n- Reusing test fixtures across similar tests\n- Using `with_defaults` more consistently (which already exists but creates new `Float` objects each time)\n- Implementing object pooling for frequently instantiated curve functions in tests\n</start_of_answer>"}
{"question": "Why does the option class that handles algebraic field extensions coordinate between its normalization method and its domain creation method to transform extension arguments into an algebraic field domain while maintaining consistency with mutually exclusive options?", "answer": "The option class that handles algebraic field extensions (the `Extension` class in `sympy/polys/polyoptions.py`) coordinates between its normalization method (`preprocess`) and its domain creation method (`postprocess`) to ensure proper transformation of user-provided extension arguments into an algebraic field domain while maintaining consistency with mutually exclusive options.\n\n**The Coordination Mechanism:**\n\n1. **Normalization Method (`preprocess`, lines 552-567)**: This method transforms various user input formats into a consistent normalized format:\n   - If the input is `1` or `True`, it returns `True` (indicating automatic extension detection)\n   - If the input is `0` or `False`, it raises an `OptionError` (invalid argument)\n   - If the input is a single algebraic element (e.g., `sqrt(2)`), it converts it to a set: `{sqrt(2)}`\n   - If the input is an iterable (e.g., `[sqrt(2), I]`), it converts it to a set: `{sqrt(2), I}`\n   - If the input is an empty iterable, it returns `None`\n\n2. **Domain Creation Method (`postprocess`, lines 569-573)**: This method uses the normalized extension format to create the actual algebraic field domain:\n   - It checks if 'extension' is in the options and is not `True`\n   - If so, it creates an algebraic field domain using `sympy.polys.domains.QQ.algebraic_field(*options['extension'])`\n   - It sets this domain in the options dictionary: `options['domain'] = QQ.algebraic_field(*options['extension'])`\n\n**Why Coordination is Necessary:**\n\nThe coordination is essential because:\n- The `preprocess` method must normalize the input first to handle various user input formats (single values, iterables, boolean flags)\n- The `postprocess` method depends on this normalized format (a set of algebraic elements) to properly unpack and pass to `QQ.algebraic_field()`\n- Without normalization, the domain creation would fail or produce incorrect results when users provide different input formats\n\n**Maintaining Consistency with Mutually Exclusive Options:**\n\nThe class maintains consistency through its `excludes` list (lines 549-550), which specifies options that cannot be used together with Extension:\n- `'greedy'`, `'domain'`, `'split'`, `'gaussian'`, `'modulus'`, `'symmetric'`\n\nThe `Options` class enforces this consistency in its `__init__` method (lines 180-182):\n```python\nfor exclude_option in cls.excludes:\n    if self.get(exclude_option) is not None:\n        raise OptionError(\"'%s' option is not allowed together with '%s'\" % (option, exclude_option))\n```\n\nThis ensures that Extension cannot be used simultaneously with other domain-defining options, preventing conflicts and maintaining the integrity of the domain specification. The coordination between `preprocess` and `postprocess` ensures that when Extension is used, it properly transforms user input into a valid algebraic field domain, while the exclusion mechanism prevents incompatible option combinations."}
{"question": "Why does the method that converts the Catalan constant singleton class to a multiple-precision floating-point value increase the precision parameter by ten bits before calling the fixed-precision computation function?", "answer": "The method increases precision by ten bits as guard digits to prevent accuracy loss during rounding in the conversion and normalization steps.\n\nImplementation pattern:\n1. The method calls `mlib.catalan_fixed(prec + 10)` to compute at higher precision.\n2. It converts the integer mantissa to an mpf using `mlib.from_man_exp(v, -prec - 10)`.\n3. It normalizes back to the requested precision using `mpf_norm(rv, prec)`.\n\nWhy extra precision is needed:\n- `catalan_fixed` returns an integer mantissa that must be converted via `from_man_exp`.\n- `mpf_norm` rounds/normalizes to the target precision, which can lose precision.\n- The extra 10 bits act as guard digits to ensure the final result at `prec` is accurate after rounding.\n\nEvidence in codebase:\n- In `sympy/core/numbers.py` lines 4051-4055, the Catalan class uses this pattern with the comment `# XXX track down why this has to be increased`.\n- The same pattern appears in `EulerGamma` (lines 3999-4003) and `GoldenRatio` (lines 3862-3865), suggesting a common need for guard digits when converting from fixed-precision integer mantissas.\n- In contrast, `Pi` and `E` (lines 3805-3806, 3642-3643) use `mpf_pi(prec)` and `mpf_e(prec)`, which return mpf values directly and do not require extra precision.\n- The codebase elsewhere uses guard digits for similar reasons, e.g., in `sympy/polys/polytools.py` line 3735: \"We need to add extra precision to guard against losing accuracy.\"\n\nThe value 10 appears to be an empirically chosen guard digit count that balances accuracy and performance for these mathematical constants."}
{"question": "Where does the differential geometry vector field commutator class determine whether to compute the result immediately versus deferring it as an unevaluated expression?", "answer": "The differential geometry vector field commutator class determines whether to compute the result immediately versus deferring it as an unevaluated expression in the `__new__` method at line 1171 of `./sympy/diffgeom/diffgeom.py`.\n\nThe decision point is the conditional check `if len(coord_sys) == 1:` (line 1171), which evaluates whether both vector fields belong to the same coordinate system. This check follows the extraction of coordinate systems from both vector fields using `_find_coords()` (line 1170), which collects all coordinate systems by finding `BaseScalarField` and `BaseVectorField` atoms in each vector field expression and extracting their `_coord_sys` attributes.\n\n**Immediate computation branch** (when `len(coord_sys) == 1`):\n- Lines 1174-1175: First checks if both arguments are `BaseVectorField` instances; if so, returns `S.Zero` immediately.\n- Lines 1176-1177: Extracts basis vectors using `atoms(BaseVectorField)` for both vector fields, creating lists `bases_1` and `bases_2`.\n- Lines 1178-1179: Extracts coefficients by calling `expand().coeff(b)` for each basis vector `b` in the respective lists, creating `coeffs_1` and `coeffs_2`.\n- Lines 1180-1183: Applies basis vectors to coefficients in a nested loop: for each pair `(c1, b1)` and `(c2, b2)`, computes `c1*b1(c2)*b2 - c2*b2(c1)*b1`, where `b1(c2)` and `b2(c1)` represent the basis vector fields acting on the coefficient expressions.\n- Line 1184: Returns the computed result `res` immediately.\n\n**Deferred/unevaluated branch** (when `len(coord_sys) != 1`):\n- Lines 1185-1189: Creates and returns an unevaluated `Commutator` object by calling `super().__new__(cls, v1, v2)`, which stores the vector fields as arguments without computing the commutator result.\n\nThe coefficient extraction stage (lines 1178-1179) uses `expand().coeff(b)` to extract scalar coefficients from the expanded vector field expressions relative to each basis vector. The basis vector application stage (lines 1181-1183) applies the basis vector fields as differential operators to the coefficients, computing the commutator formula `c1*b1(c2)*b2 - c2*b2(c1)*b1` for all combinations of basis vectors and their coefficients."}
{"question": "Where does data flow through the helper that computes maximum derivative orders for each dependent variable when the recursive order-finding utility is called for each equation-function pair?", "answer": "The data flow through `_get_func_order` when `ode_order` is called for each equation-function pair follows this path:\n\n**1. Entry Point (`_get_func_order` in `systems.py:28-29`):**\n```python\ndef _get_func_order(eqs, funcs):\n    return {func: max(ode_order(eq, func) for eq in eqs) for func in funcs}\n```\n\n**2. Data Flow Structure:**\n- **Input**: `_get_func_order` receives two parameters:\n  - `eqs`: a list of differential equations (expressions or Equality objects)\n  - `funcs`: a list of applied functions (dependent variables)\n\n- **Outer Loop**: For each function `func` in `funcs`:\n  - **Inner Loop**: For each equation `eq` in `eqs`:\n    - **Call to `ode_order`**: `ode_order(eq, func)` is invoked\n    - **Recursive Traversal**: `ode_order` (defined in `deutils.py:94-131`) recursively traverses the expression tree:\n      - If the expression matches a wildcard pattern (no function present), returns 0\n      - If it encounters a `Derivative` node:\n        - If the derivative is of the target function, returns the number of variables (order)\n        - Otherwise, recursively processes the derivative's arguments and adds the derivative's order\n      - For other expression types, recursively processes all arguments and returns the maximum order found\n    - **Result Collection**: Each `ode_order(eq, func)` call returns an integer (the order)\n  \n- **Aggregation**: For each function, `max()` computes the maximum order across all equations\n- **Output**: Returns a dictionary `{func: max_order}` mapping each function to its maximum derivative order\n\n**3. Usage Contexts:**\nThe results from `_get_func_order` flow to:\n- **`canonical_odes` (line 1069)**: Uses the order dictionary to extract highest-order derivatives when converting equations to canonical form\n- **`_classify_linear_system` (line 1413)**: Uses the order dictionary to determine system characteristics (first-order, second-order, higher-order) for classification\n\n**4. Recursive Execution Path in `ode_order`:**\nWhen `ode_order(expr, func)` is called:\n- Base case: If expression matches pattern without the function → return 0\n- Derivative case: If `Derivative` node found:\n  - Direct match: `expr.args[0] == func` → return `len(expr.variables)`\n  - Nested case: Recursively process arguments and add derivative order\n- General case: Recursively process all sub-expressions and return maximum order\n\nThe data flows from the top-level dictionary comprehension through nested generator expressions, into recursive function calls that traverse expression trees, and back up as integer order values that are aggregated into the final dictionary result.\n</start_of_answer>"}
{"question": "Where does the data flow from the deprecated matrix entry method through the matrix determinant class method to the determinant computation module?", "answer": "The data flows through the following path:\n\n1. **Starting point**: The deprecated `minorEntry` method (defined in `MatrixDeprecated` class in `sympy/matrices/matrices.py` at line 675) simply calls `self.minor(i, j, method=method)`, passing through to the `minor` method.\n\n2. **Matrix determinant class method**: The `minor` method (defined in `MatrixDeterminant` class in `sympy/matrices/matrices.py` at line 87-88) calls `_minor(self, i, j, method=method)`, which is imported from the determinant module.\n\n3. **Determinant computation module**: The `_minor` function (defined in `sympy/matrices/determinant.py` at lines 970-1004) performs the actual minor computation by:\n   - Calling `M.minor_submatrix(i, j)` to obtain the submatrix\n   - Then calling `.det(method=method)` on that submatrix\n\n4. **Index transformations in `minor_submatrix`**: The `_minor_submatrix` function (lines 1007-1046 in `determinant.py`) performs the following transformations to matrix indices:\n   - **Negative index normalization**: If `i < 0`, it adds `M.rows` to `i`; if `j < 0`, it adds `M.cols` to `j` (lines 1034-1037)\n   - **Exclusion list creation**: Creates lists of all row indices except `i` and all column indices except `j` (lines 1043-1044): `rows = [a for a in range(M.rows) if a != i]` and `cols = [a for a in range(M.cols) if a != j]`\n   - **Submatrix extraction**: Calls `M.extract(rows, cols)` to extract the submatrix (line 1046)\n\n5. **Additional index normalization in `extract`**: The `extract` method (in `sympy/matrices/matrixbase.py` at lines 468-528) further normalizes indices using the `a2idx` function (line 525-526), which:\n   - Converts negative indices to positive by adding the dimension size\n   - Validates that indices are within the valid range\n\n6. **Final computation**: The extracted submatrix then has its determinant computed using the specified method (bareiss, berkowitz, bird, laplace, or lu) via the `.det(method=method)` call.\n\nThe intermediate transformations ensure that negative indices are properly normalized and that the correct rows and columns are excluded before the submatrix is extracted and its determinant is computed."}
{"question": "Where does the function that diagonalizes vectors instantiate the matrix expression class that converts vectors to diagonal matrices?", "answer": "The matrix expression class that converts vectors to diagonal matrices (`DiagMatrix`) is instantiated at line 220 of `./sympy/matrices/expressions/diagonal.py`, within the function that diagonalizes vectors (`diagonalize_vector`).\n\nThe instantiation occurs in the return statement:\n```python\ndef diagonalize_vector(vector):\n    return DiagMatrix(vector).doit()\n```\n\nThe instantiation immediately calls `.doit()` on the new `DiagMatrix` instance. The `doit` method (defined at lines 194-216 in the same class) evaluates and simplifies the expression. It can:\n- Return the vector directly if it's already diagonal (line 203)\n- Convert explicit matrix bases to diagonal matrices (lines 204-208)\n- Handle matrix multiplication expressions by recursively calling `doit()` on nested components (line 213)\n- Handle transpose operations (lines 214-216)\n- Return a new `DiagMatrix` instance if no simplification applies (line 216)\n\nThis creates a chain where `diagonalize_vector` creates a `DiagMatrix` instance and immediately invokes `doit()`, which may recursively create additional `DiagMatrix` instances and call `doit()` again (as seen at line 213), forming the invocation chain."}
{"question": "Where in the KanesMethod class do implicit and explicit forms diverge when extracting kinematic differential equation coefficient matrices?", "answer": "The implicit and explicit forms diverge during coefficient matrix extraction in the `_initialize_kindiffeq_matrices` method of the KanesMethod class, located in `./sympy/physics/mechanics/kane.py` at lines 333-404.\n\n**Initial Extraction (Lines 364-366):**\nBoth forms start by extracting the same coefficient matrices from the kinematic differential equations using Jacobian operations:\n- `k_ku = kdeqs.jacobian(u)` - coefficient matrix for generalized speeds u\n- `k_kqdot = kdeqs.jacobian(qdot)` - coefficient matrix for time derivatives of coordinates q'\n- `f_k = kdeqs.xreplace(u_zero).xreplace(qdot_zero)` - forcing term\n\nThese extract the linear coefficients from equations in the form: `k_ku(q,t)*u(t) + k_kqdot(q,t)*q'(t) + f_k(q,t) = 0`\n\n**Divergence Point (Lines 379-397):**\n\n**Implicit Form (Lines 379-381):**\nStores the original extracted matrices directly without solving:\n- `self._f_k_implicit = f_k.xreplace(uaux_zero)`\n- `self._k_ku_implicit = k_ku.xreplace(uaux_zero)`\n- `self._k_kqdot_implicit = k_kqdot` (original matrix, typically not identity)\n\n**Explicit Form (Lines 391-397):**\nSolves the system to express q' explicitly in terms of u by premultiplying by `k_kqdot^-1`:\n- `f_k_explicit = linear_solver(k_kqdot, f_k)` - solves for q' forcing term\n- `k_ku_explicit = linear_solver(k_kqdot, k_ku)` - solves for q' coefficient\n- `self._k_kqdot = eye(len(qdot))` - becomes identity matrix\n- `self._f_k = f_k_explicit.xreplace(uaux_zero)`\n- `self._k_ku = k_ku_explicit.xreplace(uaux_zero)`\n\nThe explicit form transforms the equation to: `I*q'(t) + (k_kqdot^-1*k_ku)*u(t) + (k_kqdot^-1*f_k) = 0`, where the mass matrix becomes identity.\n\n**Usage (Lines 795-805):**\nThe `explicit_kinematics` attribute (set during initialization, line 230) determines which form is used:\n- `mass_matrix_kin` property returns `_k_kqdot` (identity) for explicit or `_k_kqdot_implicit` (original matrix) for implicit\n- `forcing_kin` property uses the corresponding `_k_ku` and `_f_k` matrices based on the flag\n\nThe divergence occurs because the explicit form solves the kinematic differential equations at initialization (line 393 creates `_qdot_u_map`), while the implicit form preserves the original unsolved matrix structure, allowing more compact representation for non-simple kinematics (as noted in the class docstring, lines 111-117)."}
{"question": "Where does the ket representation class propagate bracket and LaTeX formatting attributes through the inheritance hierarchy from the base many-particle Fock state class?", "answer": "The ket representation class propagates bracket and LaTeX formatting attributes through the inheritance hierarchy via class attributes defined in the subclass and dynamic attribute lookup in the base class methods.\n\nSpecifically, in `./sympy/physics/secondquant.py`:\n\n1. **Base class mechanism** (lines 966, 975-976): The base many-particle Fock state class (`FockState`) defines `__str__` and `_latex` methods that use `getattr(self, 'lbracket', \"\")`, `getattr(self, 'rbracket', \"\")`, `getattr(self, 'lbracket_latex', \"\")`, and `getattr(self, 'rbracket_latex', \"\")` to dynamically look up formatting attributes.\n\n2. **Subclass attribute definition** (lines 1236-1239): The ket representation class (`FockStateKet`) inherits from the base class and defines these attributes as class attributes:\n   - `lbracket = '|'`\n   - `rbracket = '>'`\n   - `lbracket_latex = r'\\left|'`\n   - `rbracket_latex = r'\\right\\rangle'`\n\n3. **Propagation path**: When an instance of `FockStateKet` (or its subclasses like `FockStateBosonKet` or `FockStateFermionKet`) calls `__str__` or `_latex`, Python's attribute lookup follows the Method Resolution Order (MRO). The `getattr()` calls in the base class methods find the class attributes defined in `FockStateKet`, which are inherited by all subclasses.\n\n4. **Control flow preservation**: When state transformations occur (via `up()`, `down()`, or operator applications), methods use `self.__class__(...)` to create new instances (see lines 1001, 1023, 1197, 1199, 1205), preserving the class type and thus the formatting attributes. The `getattr()` mechanism ensures these attributes are correctly applied at runtime regardless of the specific subclass in the hierarchy.\n\nThis design allows the base class to provide formatting logic while subclasses define the specific bracket characters, enabling polymorphic behavior where different state types (ket vs bra, boson vs fermion) display with appropriate formatting."}
{"question": "Where are the lower-level helper functions located that the evaluation method of the Dirichlet eta function class delegates to for computing zeta function values and digamma transformations?", "answer": "The `dirichlet_eta.eval` method delegates to helper functions in two modules:\n\n1. For computing zeta function values:\n   - The `zeta` class is defined in the same file as `dirichlet_eta`: `sympy/functions/special/zeta_functions.py` (line 395). The `eval` method calls `zeta(s)` on line 638, `zeta(s, a)` on line 645, and `zeta(s, (a+1)/2)` on line 646.\n\n2. For digamma transformations:\n   - The `digamma` class is defined in `sympy/functions/special/gamma_functions.py` (line 1060). The `eval` method calls `digamma(a)` and `digamma((a+1)/2)` on line 644.\n   - The `digamma.eval` method (line 1135-1136) delegates to `polygamma(0, z)`, which is also defined in `sympy/functions/special/gamma_functions.py` (line 564), making `polygamma` a lower-level helper for digamma computations.\n\nSummary:\n- `zeta`: `sympy/functions/special/zeta_functions.py` (line 395)\n- `digamma`: `sympy/functions/special/gamma_functions.py` (line 1060)\n- `polygamma`: `sympy/functions/special/gamma_functions.py` (line 564)"}
{"question": "Where in the generalized inertia force method are mass and non-mass matrix components computed separately for rigid bodies versus particles, with dependent speed transformation applied?", "answer": "In the `_form_frstar` method (lines 437-534 in `./sympy/physics/mechanics/kane.py`), the mass matrix (MM) and non-mass matrix (nonMM) components are computed separately for rigid bodies versus particles in the main loop (lines 480-510).\n\n**Separate computation for rigid bodies vs particles:**\n\nFor **RigidBody** objects (lines 481-500):\n- Mass matrix contributions are computed for both translational and rotational components:\n  - Translational: `MM[j, k] += M*tmp_vel.dot(partials[i][0][k])` (line 496)\n  - Rotational: `MM[j, k] += tmp_ang.dot(partials[i][1][k])` (line 498)\n- Non-mass matrix contributions include both:\n  - Translational: `nonMM[j] += inertial_force.dot(partials[i][0][j])` (line 499)\n  - Rotational: `nonMM[j] += inertial_torque.dot(partials[i][1][j])` (line 500)\n\nFor **Particle** objects (lines 501-510):\n- Mass matrix contributions are computed only for translational components:\n  - `MM[j, k] += M*temp.dot(partials[i][0][k])` (line 509)\n- Non-mass matrix contributions are computed only for translational components:\n  - `nonMM[j] += inertial_force.dot(partials[i][0][j])` (line 510)\n\n**Dependent speed transformation application:**\n\nAfter the loop completes and `fr_star` is composed from MM and nonMM (line 515), if dependent speeds exist (lines 518-528), the dependent speed transformation matrix `_Ars` is applied **separately** to both components:\n\n1. **Mass matrix transformation** (lines 524-526):\n   - The mass matrix is partitioned into independent (`MMi = MM[:p, :]`) and dependent (`MMd = MM[p:o, :]`) parts\n   - Transformation applied: `MM = MMi + (self._Ars.T * MMd)`\n\n2. **Non-mass matrix transformation** (line 528):\n   - The non-mass matrix is partitioned into independent and dependent parts\n   - Transformation applied: `nonMM = nonMM[:p, :] + (self._Ars.T * nonMM[p:o, :])`\n\nThe `_Ars` matrix (computed in `_initialize_constraint_matrices` at line 325) relates dependent speeds to independent speeds through the velocity constraints, and its transpose is used to transform both the mass matrix and non-mass matrix components independently when eliminating dependent speeds from the equations of motion."}
{"question": "Where does the Kane's method class conditionally create a secondary instance when auxiliary speeds are present, transferring the coordinate-to-speed mapping from the original instance?", "answer": "The Kane's method class conditionally creates a secondary instance in the `kanes_equations` method when auxiliary generalized speeds (`_uaux`) are present. This occurs at lines 719-732 in `./sympy/physics/mechanics/kane.py`.\n\nThe conditional creation happens after the primary instance computes `fr` (generalized active force) and `frstar` (generalized inertia force) at lines 717-718. When `self._uaux` is truthy (line 719), the code creates a secondary `KanesMethod` instance (`km`) with different parameters depending on whether dependent speeds exist:\n\n- If no dependent speeds (`not self._udep`): The secondary instance is created with the inertial frame, coordinates (`self.q`), auxiliary speeds as the independent speeds, and the same constraint solver (lines 721-722).\n\n- If dependent speeds exist: The secondary instance is additionally configured with dependent speeds, velocity constraints, and acceleration constraints derived from the primary instance's constraint matrices (lines 724-730).\n\nThe specific state that must be transferred is `_qdot_u_map` (line 732: `km._qdot_u_map = self._qdot_u_map`). This dictionary maps time derivatives of generalized coordinates (q') to generalized speeds (u). It is computed during `_initialize_kindiffeq_matrices` (line 393) by solving the kinematic differential equations to express q' in terms of u.\n\nThe transfer is necessary because, as documented in the comment at lines 295-299, when auxiliary speeds are present, the secondary instance is created without kinematic differential equations (`kdeqs`). Since `_qdot_u_map` is only computed when `kdeqs` are provided during initialization (see `_initialize_kindiffeq_matrices` at lines 333-403), the secondary instance would not have this mapping available. Without this transfer, the secondary instance's methods like `_form_fr` and `_form_frstar` (called at lines 734-735) would fail when they try to use `_qdot_u_map` for substitutions (see lines 414-415, 469, etc.), as the mapping is essential for converting expressions involving q' into expressions involving u."}
{"question": "Where is the method that checks whether both coefficient terms in a first-order differential equation are homogeneous functions of the same order located?", "answer": "The verification logic is located in the `_verify` methods of three solver classes in the file `./sympy/solvers/ode/single.py`:\n\n1. **`HomogeneousCoeffSubsDepDivIndep._verify()`** (lines 1510-1523): Verifies equations suitable for the substitution `u = y/x` (dependent divided by independent variable)\n\n2. **`HomogeneousCoeffSubsIndepDivDep._verify()`** (lines 1637-1650): Verifies equations suitable for the substitution `u = x/y` (independent divided by dependent variable)\n\n3. **`HomogeneousCoeffBest._verify()`** (lines 1714-1716): Combines both verification methods, requiring that both substitutions are valid\n\nThe specific homogeneity conditions enforced are:\n\n1. **Same homogeneous order requirement**: Both coefficient terms (denoted as `d` and `e` in the equation `d + e*f'(x) = 0`) must be homogeneous functions of the same order. This is checked using the `homogeneous_order()` function (defined in `./sympy/solvers/ode/ode.py` at lines 2196-2284), which determines if a function `F(x, y)` satisfies `F(tx, ty) = t^n * F(x, y)` for some order `n`. Both `ordera = homogeneous_order(self.d, x, self.y)` and `orderb = homogeneous_order(self.e, x, self.y)` must return the same non-None value.\n\n2. **Non-zero substitution check for `HomogeneousCoeffSubsDepDivIndep`**: After confirming equal orders, the method verifies that `(self.d + self.u*self.e).subs({x: 1, self.y: self.u}) != 0`, where `u` is a dummy variable. This ensures the substitution `y = u*x` doesn't result in a degenerate case.\n\n3. **Non-zero substitution check for `HomogeneousCoeffSubsIndepDivDep`**: Similarly, it verifies that `(self.e + self.u*self.d).subs({x: self.u, self.y: 1}) != 0`, ensuring the substitution `x = u*y` is valid.\n\nThe verification process first separates variables using `separatevars()`, substitutes the dependent variable with a dummy `y`, then checks homogeneity orders. If both coefficients have the same non-None homogeneous order, it performs the additional non-zero check before returning `True`. If any condition fails, it returns `False`."}
{"question": "Where is the method that matches first-order differential equations against a template pattern to extract two coefficient expressions implemented?", "answer": "The pattern matching mechanism that extracts coefficients P and Q from differential equations is implemented in the `_matches` method of the `SinglePatternODESolver` class, located in `./sympy/solvers/ode/single.py` at lines 303-322.\n\nThe implementation works as follows:\n\n1. **Pattern Creation**: Subclasses implement `_wilds` to create `Wild` objects (typically named P and Q) with `exclude` parameters. For example:\n   - `FirstExact._wilds` (lines 498-501): Creates `P = Wild('P', exclude=[f(x).diff(x)])` and `Q = Wild('Q', exclude=[f(x).diff(x)])`\n   - `FirstLinear._wilds` (lines 622-625): Creates `P = Wild('P', exclude=[f(x)])` and `Q = Wild('Q', exclude=[f(x), f(x).diff(x)])`\n   - `AlmostLinear._wilds` (lines 692-695): Creates `P = Wild('P', exclude=[f(x).diff(x)])` and `Q = Wild('Q', exclude=[f(x).diff(x)])`\n\n2. **Pattern Matching**: The `_matches` method (lines 303-322) constructs a pattern using `_equation`, collects terms, and calls `eq.match(pattern)` at line 319 to extract the coefficients.\n\n3. **Exclusion Criteria**: The exclusion is enforced in the `Wild.matches` method in `sympy/core/symbol.py` at line 663, which checks `if any(expr.has(x) for x in self.exclude): return None`. This prevents a wildcard from matching any expression containing the excluded terms. For instance, when `P = Wild('P', exclude=[f(x).diff(x)])`, P will not match expressions containing the first derivative, ensuring it only captures the coefficient of the non-derivative term."}
{"question": "Where do modules and classes directly instantiate the expression wrapper class that prevents automatic evaluation?", "answer": "Based on my exploration of the SymPy codebase, modules and classes directly instantiate `UnevaluatedExpr` in the following locations:\n\n1. **Test files** (multiple locations):\n   - `sympy/printing/tests/test_str.py` (line 1059): `2*UnevaluatedExpr(a+b)` in `test_UnevaluatedExpr()`\n   - `sympy/printing/tests/test_latex.py` (line 2536): `UnevaluatedExpr(1/x)` in `test_latex_UnevaluatedExpr()`\n   - `sympy/printing/tests/test_c.py` (lines 871-877): Multiple instantiations in `test_ccode_UnevaluatedExpr()`\n   - `sympy/printing/tests/test_fortran.py` (lines 37, 41): Instantiations in `test_UnevaluatedExpr()`\n   - `sympy/printing/pretty/tests/test_pretty.py` (line 7117): `UnevaluatedExpr(1/x)` in `test_pretty_UnevaluatedExpr()`\n   - `sympy/utilities/tests/test_lambdify.py` (line 144): `UnevaluatedExpr(q+r)` in a test\n   - `sympy/simplify/tests/test_cse.py` (line 549): `UnevaluatedExpr(x + 1)` in `test_cse_unevaluated()`\n   - `sympy/vector/tests/test_printing.py` (line 201): `UnevaluatedExpr(10)` in a test\n   - `sympy/physics/biomechanics/tests/test_curve.py` (multiple lines): Used in assertions for expected values\n   - `sympy/physics/biomechanics/tests/test_musculotendon.py`: Used in tests\n\n2. **Production code**:\n   - `sympy/physics/biomechanics/curve.py` (multiple locations, lines 250, 266, 268, 270, 274, 469, 493, 685, 701, 704, 705, 709, 899, 1189, 1190, 1191, 1465, 1481, 1485, 1488, 1490, 1689): Extensively used in biomechanical curve functions like `TendonForceLengthDeGroote2016`, `TendonForceLengthInverseDeGroote2016`, `FiberForceLengthPassiveDeGroote2016`, `FiberForceLengthActiveDeGroote2016`, `FiberForceVelocityDeGroote2016`, and `FiberForceVelocityInverseDeGroote2016` to prevent evaluation of subexpressions when `evaluate=False` is passed\n\n3. **Code generation/printing modules**:\n   - `sympy/printing/codeprinter.py` (line 124): The `_handle_UnevaluatedExpr` method processes `UnevaluatedExpr` instances, but doesn't instantiate them directly\n\nThe class is defined in `sympy/core/expr.py` (line 4096) and is imported in various modules via `from sympy.core.expr import UnevaluatedExpr` or `from sympy import UnevaluatedExpr`. The primary production use is in the biomechanics module, where it's used to preserve unevaluated expressions in mathematical formulas for biomechanical modeling."}
